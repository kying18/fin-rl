{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "institutional-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-classroom",
   "metadata": {},
   "source": [
    "# Architecture for ACModel, Rollouts, and PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "seeing-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def init_gru_params(gru):\n",
    "    for name, param in gru.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.orthogonal_(param)\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "        \n",
    "#         print(self.logstd.sum())\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        output = torch.distributions.Normal(mean, std)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_tickers, time_horizon, num_ta_indicators, recurrent, num_layers=1, hidden_size=64, learn_std=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        self.num_tickers = num_tickers\n",
    "        self.time_horizon = time_horizon\n",
    "        self.num_ta_indicators = num_ta_indicators\n",
    "        \n",
    "        # TODO changed for cartpole\n",
    "        # action_dim = num_tickers + 1 # buy/sell for each ticker + 1 for cash\n",
    "        action_dim = 1\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            num_inputs = num_tickers\n",
    "            \n",
    "            self.gru_actor = nn.GRU(num_inputs, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "            self.gru_critic = nn.GRU(num_inputs, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "            \n",
    "            init_gru_params(self.gru_actor)\n",
    "            init_gru_params(self.gru_critic)\n",
    "        else:\n",
    "            num_inputs = num_tickers * time_horizon\n",
    "            \n",
    "            seq = [nn.Linear(num_inputs, hidden_size), nn.Tanh()]\n",
    "            for i in range(num_layers-1):\n",
    "                seq.extend([nn.Linear(hidden_size, hidden_size), nn.Tanh()])\n",
    "            \n",
    "            self.fwd_actor = nn.Sequential(*seq)\n",
    "            self.fwd_critic = nn.Sequential(*seq)\n",
    "        \n",
    "        # output of gru/fwd, cash, holdings, and then all the ta indicators\n",
    "        # TODO changed for cartpole\n",
    "        # num_inputs = hidden_size + 1 + num_tickers + num_tickers * num_ta_indicators\n",
    "        num_inputs = hidden_size\n",
    "        \n",
    "        mid_layer = int(hidden_size/2)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, mid_layer), nn.Tanh(),\n",
    "            nn.Linear(mid_layer, action_dim), nn.Tanh(),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, mid_layer), nn.Tanh(),\n",
    "            nn.Linear(mid_layer, 1)\n",
    "        )\n",
    "        \n",
    "        self.dist = DiagonalGaussian(action_dim, learn_std=learn_std)\n",
    "\n",
    "        self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h_a=None, rnn_h_c=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        \n",
    "        price_obs = obs[:,:self.num_tickers * self.time_horizon]\n",
    "        other_obs = obs[:,self.num_tickers * self.time_horizon:]\n",
    "        \n",
    "#         price_obs = torch.zeros(price_obs_in.shape)\n",
    "        \n",
    "#         price_obs[:,1::4] = torch.sin(price_obs_in[:,1::4])\n",
    "\n",
    "#         price_obs[:,0::4] = price_obs_in[:,0::4]\n",
    "\n",
    "        \n",
    "        if self.recurrent:\n",
    "            if rnn_h_a is None:\n",
    "                rnn_h_a = self.init_hidden(obs.size(0))\n",
    "            if rnn_h_c is None:\n",
    "                rnn_h_c = self.init_hidden(obs.size(0))\n",
    "            \n",
    "            obs = torch.reshape(price_obs, (-1, self.time_horizon, self.num_tickers))\n",
    "            obs_actor, rnn_h_a = self.gru_actor(obs, rnn_h_a)\n",
    "\n",
    "            obs_actor = obs_actor[:,-1,:] # selecting the last element\n",
    "            \n",
    "            obs_critic, rnn_h_c = self.gru_critic(obs, rnn_h_c)\n",
    "            obs_critic = obs_critic[:,-1,:] # selecting the last element\n",
    "        else:\n",
    "            obs_actor = self.fwd_actor(price_obs)\n",
    "            obs_critic = self.fwd_critic(price_obs)\n",
    "            \n",
    "#         obs_actor = torch.cat((obs_actor, other_obs), 1)\n",
    "#         obs_critic = torch.cat((obs_critic, other_obs), 1)\n",
    "        \n",
    "        forward_actor = self.actor(obs_actor)\n",
    "        action_dist = self.dist.evaluate(forward_actor)\n",
    "        \n",
    "        forward_critic = self.critic(obs_critic)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h_a, rnn_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fuzzy-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.99, gae_lambda=0.95, device=None):\n",
    "        # TODO changed for cartpole\n",
    "        # self.episode_length = env.episode_length\n",
    "        self.episode_length = 300 - 1\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        self.returns = None\n",
    "        \n",
    "        self.num_rollouts = 3\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.values = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.returns = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        \n",
    "        self.log_probs = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.obss = [None] * (self.episode_length*self.num_rollouts)\n",
    "        \n",
    "        self.gaes = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        \n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        if isinstance(obs, list):\n",
    "            obs = np.stack(obs)\n",
    "\n",
    "        if len(obs.shape) == 1: # 1 dimensional\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            \n",
    "        return torch.FloatTensor(obs)\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        \n",
    "        total_return = 0\n",
    "        self.returns = []\n",
    "        \n",
    "        for ep in range(self.num_rollouts):\n",
    "            obs = env.reset()\n",
    "            \n",
    "            self.actions_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            self.values_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            self.rewards_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            self.obss_ = [None] * (self.episode_length)\n",
    "            self.log_probs_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            \n",
    "            \n",
    "            T = 0\n",
    "        \n",
    "            while True:\n",
    "                with torch.no_grad():\n",
    "                    dist, value, _, _ = self.acmodel(self.process_obs(obs))\n",
    "\n",
    "                action = dist.sample()\n",
    "\n",
    "                self.obss_[T] = obs\n",
    "\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_return += reward\n",
    "\n",
    "                self.actions_[T] = action\n",
    "                self.values_[T] = value\n",
    "                self.rewards_[T] = float(reward)\n",
    "                self.log_probs_[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "                T += 1\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            self.actions[ep*self.episode_length:(ep+1)*self.episode_length] = self.actions_[:T]\n",
    "            self.values[ep*self.episode_length:(ep+1)*self.episode_length] = self.values_[:T]\n",
    "            self.rewards[ep*self.episode_length:(ep+1)*self.episode_length] = self.rewards_[:T]\n",
    "            \n",
    "            \n",
    "            discounted_reward = 0.0\n",
    "            self.returns_ = []\n",
    "            for r in reversed(self.rewards_):\n",
    "                discounted_reward = r + self.discount*discounted_reward\n",
    "                self.returns_.insert(0, discounted_reward)\n",
    "            self.returns[ep*self.episode_length:(ep+1)*self.episode_length] = self.returns_[:T]\n",
    "            \n",
    "\n",
    "            self.log_probs[ep*self.episode_length:(ep+1)*self.episode_length] = self.log_probs_[:T]\n",
    "            \n",
    "            self.obss[ep*self.episode_length:(ep+1)*self.episode_length] = self.process_obs(self.obss_[:T])\n",
    "            self.gaes_ = self.compute_advantage_gae(self.rewards_, self.values_, T)\n",
    "            self.gaes[ep*self.episode_length:(ep+1)*self.episode_length] = self.gaes_[:T]\n",
    "            \n",
    "        \n",
    "        self.obss = torch.FloatTensor(np.stack(self.obss))\n",
    "        self.returns = torch.FloatTensor(self.returns)\n",
    "        \n",
    "        #self.gaes = (self.gaes - self.gaes.mean())/(self.gaes.std() + 1e-8)\n",
    "        \n",
    "        return total_return/self.num_rollouts, T\n",
    "            \n",
    "    def compute_advantage_gae(self, rewards, values, T):\n",
    "#         def _delta(t, rewards, discount, values):\n",
    "#             return rewards[t] + ((discount * values[t+1] - values[t]) if t+1 < values.shape[0] else 0)\n",
    "\n",
    "#         advantages = torch.zeros_like(values)\n",
    "\n",
    "#         n = values.shape[0]\n",
    "#         for t in range(n):\n",
    "#             advantages[t] = sum([(self.gae_lambda*self.discount)**i * _delta(t+i, rewards, self.discount, values) for i in range(n-t)])\n",
    "        \n",
    "# #         advs = advantages[:T]\n",
    "# #         return (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "\n",
    "#         print (advantages)\n",
    "#         return advantages[:T]\n",
    "\n",
    "        deltas = torch.cat((rewards[:-1] + self.discount*values[1:] - values[:-1], rewards[-1:] - values[-1:]))\n",
    "        deltas_flip = torch.flip(deltas, [0])\n",
    "        A = 0.0*deltas_flip[0]\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in range(len(deltas_flip)):\n",
    "            A = self.discount*self.gae_lambda*A + deltas_flip[i]\n",
    "            advantages[i] = A\n",
    "        advantages = torch.flip(advantages, [0])\n",
    "        return advantages\n",
    "            \n",
    "            \n",
    "            \n",
    "        coeffs = torch.pow(self.discount*self.gae_lambda, torch.arange(len(deltas)))\n",
    "        return torch.cumsum(torch.multiply(deltas, coeffs), -1)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "satisfactory-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=2e-4,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=10):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "\n",
    "        dist, _, _, _ = self.acmodel(rollouts.obss) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions.view(-1,self.acmodel.action_dim)).detach()\n",
    "        \n",
    "\n",
    "#         policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "#         value_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "        \n",
    "        batch_size = 64\n",
    "        avg_policy_loss, avg_value_loss = 0, 0\n",
    "        for i in range(self.train_iters):\n",
    "            rand_idxs = np.random.choice(np.arange(len(rollouts.obss)), batch_size , replace = False)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss[rand_idxs], old_logp[rand_idxs], \n",
    "                                                               rollouts.actions[rand_idxs], rollouts.gaes[rand_idxs])\n",
    "\n",
    "            v_loss = self._compute_value_loss(rollouts.obss[rand_idxs], rollouts.returns[rand_idxs])\n",
    "            #print ('vloss:', v_loss, 'pi_loss:', pi_loss)\n",
    "            loss = .5*v_loss + pi_loss\n",
    "            \n",
    "            avg_policy_loss += pi_loss.item()\n",
    "            avg_value_loss += v_loss.item()\n",
    "   \n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "            \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "            #loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(acmodel.parameters(), .5)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return avg_policy_loss/self.train_iters, avg_value_loss/self.train_iters\n",
    "        \n",
    "    def _compute_policy_loss_ppo(self, obs, old_logp, actions, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "        \n",
    "\n",
    "        dist, _, _, _ = self.acmodel(obs)\n",
    "        \n",
    "        new_logp = dist.log_prob(actions.view(-1,self.acmodel.action_dim))\n",
    "        \n",
    "        entropy = torch.mean(dist.entropy())\n",
    "        entropy = entropy * self.entropy_coef\n",
    "\n",
    "        r = torch.exp(new_logp - old_logp.detach())\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-self.clip_ratio, 1+self.clip_ratio)*advantages.view(-1,1)\n",
    "        \n",
    "        min_advs = torch.minimum(r*advantages.view(-1,1), clamp_adv)\n",
    "        \n",
    "\n",
    "        policy_loss = -torch.mean(min_advs) - entropy\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, values, _, _ = self.acmodel(obs)\n",
    "\n",
    "        value_loss = torch.mean((returns.view(-1,1) - values.view(-1,1))**2)\n",
    "\n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-engagement",
   "metadata": {},
   "source": [
    "# Half Cheetah Gym Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "outside-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import registry, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "stunning-physiology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import utils\n",
    "from gym.envs.mujoco import mujoco_env\n",
    "\n",
    "class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n",
    "    prev_obs = [np.zeros((8,)), np.zeros((8,)), np.zeros((8,))]\n",
    "    timesteps = 0\n",
    "    timestep_limit = 300\n",
    "    time_horizon = 3\n",
    "    \n",
    "    def __init__(self):\n",
    "        mujoco_env.MujocoEnv.__init__(self, 'half_cheetah.xml', 5)\n",
    "        utils.EzPickle.__init__(self)\n",
    "#         self.prev_obs = [np.zeros((8,)), np.zeros((8,)), np.zeros((8,))]\n",
    "\n",
    "    def step(self, action):\n",
    "        xposbefore = self.sim.data.qpos[0]\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        xposafter = self.sim.data.qpos[0]\n",
    "        ob = self._get_obs()\n",
    "        reward_ctrl = - 0.1 * np.square(action).sum()\n",
    "        reward_run = (xposafter - xposbefore)/self.dt\n",
    "        reward = reward_ctrl + reward_run\n",
    "        done = self.is_done()\n",
    "        self.timesteps += 1\n",
    "        return ob, reward, done, dict(reward_run=reward_run, reward_ctrl=reward_ctrl)\n",
    "    \n",
    "    def is_done(self):\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "    def _shift_prev_obs(self, new_qpos):\n",
    "        for i in range(self.time_horizon-1):\n",
    "            self.prev_obs[i] = self.prev_obs[i+1]\n",
    "        self.prev_obs[self.time_horizon-1] = new_qpos\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        self._shift_prev_obs(self.sim.data.qpos.flat[1:])\n",
    "        return np.concatenate(self.prev_obs)\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)\n",
    "        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n",
    "        self.prev_obs = [np.zeros((8,)), np.zeros((8,)), np.zeros((8,))]\n",
    "        self.timesteps = 0\n",
    "        self.set_state(qpos, qvel)\n",
    "        \n",
    "        for t in range(self.time_horizon-1):\n",
    "            # start with a few random actions to initialize\n",
    "            action = np.zeros(1)\n",
    "            self.step(action)\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def viewer_setup(self):\n",
    "        self.viewer.cam.distance = self.model.stat.extent * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "expired-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetahWithMemory-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:HalfCheetahEnv',\n",
    ")\n",
    "env = gym.make('HalfCheetahWithMemory-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dependent-concert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.06396860e-02,  4.59685630e-02,  1.64286942e-02,  9.13984730e-03,\n",
       "       -3.14023902e-03,  7.83826334e-02,  3.25873496e-03,  2.20562009e-03,\n",
       "       -9.74352719e-02,  2.48871148e-02, -1.51446949e-02,  1.44422708e-02,\n",
       "       -5.74134440e-03,  5.27939766e-02, -2.21285536e-02, -7.08987396e-05,\n",
       "       -9.74352719e-02,  2.48871148e-02, -1.51446949e-02,  1.44422708e-02,\n",
       "       -5.74134440e-03,  5.27939766e-02, -2.21285536e-02, -7.08987396e-05])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "divided-turtle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-strain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "excessive-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.223006248474121, 1102.7235046386718)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recurrent model\n",
    "acmodel = ACModel(num_tickers=8, time_horizon=3, num_ta_indicators=0, recurrent=False, hidden_size=32, num_layers=2)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ed342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "chief-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6071253150701523, 817.0509826660157)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feedforward model\n",
    "acmodel = ACModel(num_tickers=8, time_horizon=3, num_ta_indicators=0, recurrent=False, hidden_size=32, num_layers=2)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "traditional-greensboro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([897, 24])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts.obss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-bottle",
   "metadata": {},
   "source": [
    "Ok so looks like the Cartpole gym syntactically works.. Let's try to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-optimum",
   "metadata": {},
   "source": [
    "# Run experiment on Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "selected-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 6.884 HW4\n",
    "\n",
    "def run_experiment(acmodel, ppo_kwargs, rollout_kwargs = {}, max_episodes=200000, score_threshold=0.8):\n",
    "    # acmodel_args should be dictionary corresponding to inputs of acmodel\n",
    "    # ie {num_tickers: 4, time_horizon: 5, etc..}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make('HalfCheetahWithMemory-v0')\n",
    "    \n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "    \n",
    "    SMOOTH_REWARD_WINDOW = 30\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "    \n",
    "    num_frames = 0\n",
    "    rollouts = RolloutBuffer(acmodel, env, **rollout_kwargs)\n",
    "    ppo = PPO(acmodel, **ppo_kwargs)\n",
    "    \n",
    "    lr = ppo_kwargs['lr']\n",
    "    checkpoint_path = f'./checkpoints/cheetah/acmodel_rnn{acmodel.recurrent}_{acmodel.num_layers}lay_{acmodel.hidden_size}hid_{lr}lr'\n",
    "\n",
    "    pbar = tqdm(range(max_episodes))\n",
    "    for update in pbar:\n",
    "        rollouts.reset() # resetting the buffer\n",
    "        total_return, T = rollouts.collect_experience()\n",
    "        policy_loss, value_loss = ppo.update(rollouts)\n",
    "        \n",
    "        num_frames += T\n",
    "        rewards.append(total_return)\n",
    "        \n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':total_return, 'policy_loss': policy_loss, 'value_loss': value_loss}\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # save results every 500\n",
    "        if update % 500 == 0:\n",
    "            Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(acmodel.state_dict(), f'{checkpoint_path}/{update}')\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= score_threshold:\n",
    "            is_solved = True\n",
    "            Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(acmodel.state_dict(), f'{checkpoint_path}/solved')\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "    else:\n",
    "        print('Unsolved. Check your implementation.')\n",
    "    \n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-correspondence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 446/2000 [07:56<12:47,  2.03it/s, episode=445, num_frames=133354, smooth_reward=187, reward=tensor(197.2538), policy_loss=-6.19, value_loss=1.63e+3]  "
     ]
    }
   ],
   "source": [
    "# feedforward\n",
    "fwd = []\n",
    "for i in range(2):\n",
    "    acmodel_kwargs = {'num_tickers': 8, 'time_horizon': 3, 'num_ta_indicators': 0, 'recurrent': False, 'hidden_size': 32, 'num_layers': 2}\n",
    "    acmodel = ACModel(**acmodel_kwargs)\n",
    "    ppo_kwargs = {'lr': 5e-4, 'entropy_coef': 0.01, 'train_iters': 10}\n",
    "    fwd_df = run_experiment(acmodel, ppo_kwargs, max_episodes=2000, score_threshold=2000)\n",
    "    print(\"Num eps: \", len(fwd_df))\n",
    "    ax=fwd_df.plot(x='num_frames', y=['reward', 'smooth_reward'])\n",
    "    ax.set_ylim((-20, 100))\n",
    "    plt.show()\n",
    "    fwd.append(fwd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-costa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# recurrent\n",
    "rnn = []\n",
    "for i in range(2):\n",
    "    acmodel_kwargs = {'num_tickers': 8, 'time_horizon': 3, 'num_ta_indicators': 0, 'recurrent': True, 'hidden_size': 32, 'num_layers': 2}\n",
    "    acmodel = ACModel(**acmodel_kwargs)\n",
    "    ppo_kwargs = {'lr': 5e-4, 'entropy_coef': 0.01, 'train_iters': 10}\n",
    "    rnn_df = run_experiment(acmodel, ppo_kwargs, max_episodes=2000, score_threshold=70)\n",
    "    print(\"Num eps: \", len(rnn_df))\n",
    "    ax=rnn_df.plot(x='num_frames', y=['reward', 'smooth_reward'])\n",
    "    ax.set_ylim((-20, 100))\n",
    "    plt.show()\n",
    "    rnn.append(rnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-prague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "\n",
    "style.use('seaborn-poster') #sets the size of the charts\n",
    "style.use('ggplot')\n",
    "\n",
    "for df in fwd:\n",
    "    df['label'] = \"Feedforward\"\n",
    "    \n",
    "for df in rnn:\n",
    "    df['label'] = \"Recurrent\"\n",
    "\n",
    "all_logs = []\n",
    "all_logs.extend(fwd)\n",
    "all_logs.extend(rnn)\n",
    "\n",
    "logs = pd.concat(all_logs, ignore_index=True)\n",
    "nums = len(logs['label'].unique())\n",
    "palette = sns.color_palette(\"hls\", nums)\n",
    "fig = plt.figure()\n",
    "ax = sns.lineplot(x='num_frames', y='smooth_reward', data=logs, estimator='mean', palette=palette, hue='label')\n",
    "ax.set_title('Comparing Smooth Reward')\n",
    "ax.set_xlabel('Num Frames')\n",
    "ax.set_ylabel('Smooth Reward')\n",
    "ax.set_ylim((-30, 90))\n",
    "plt.show()\n",
    "fig.savefig('ppo - 2state, 2layerac-restructured.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-restriction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
