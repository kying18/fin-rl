{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5d5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6bde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Technical indicators list. TODO: put these in a config file\n",
    "\n",
    "\n",
    "TECHNICAL_INDICATORS_LIST = [\"macd\", \"macds\",\n",
    "                             \"boll_ub\",\"boll_lb\", \n",
    "                             \"rsi_14\", \"cci_14\", \"dx_14\",\n",
    "                             \"open_14_sma\", \"pdi\", \"mdi\",\n",
    "                            \"dx\", \"adx\", \"vr\", \"wr_14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c93dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = 'perc_changes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2581be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['JNJ', 'JPM', 'DIS', 'HD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f479d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb21516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to perform softmax\n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "\n",
    "INITIAL_BALANCE = 1000.0 #Start with balance of 1000 dollars\n",
    "NUM_PAST_STATES = 15 #use past 14 days of price data as part of observation\n",
    "EPISODE_LENGTH = 30 #each episode goes for 30 trading days\n",
    "TRADE_FREQ = 1 #Trade every x amount of days\n",
    "\n",
    "#OpenAI Gym style environment for RL\n",
    "class TradeEnv(gym.Env):\n",
    "    def __init__(self, tickers):\n",
    "        super(TradeEnv, self).__init__()\n",
    "\n",
    "        self.tickers = tickers\n",
    "        \n",
    "        data_idx = np.random.randint(0, 9)\n",
    "        self.data_file = pickle.load(open('fake_data/train_ta_{}.p'.format(data_idx), 'rb'))\n",
    "        \n",
    "        self.feature_names = list(self.data_file[list(self.data_file.keys())[0]].keys())\n",
    "        self.feature_names.remove('date')\n",
    "        self.feature_names.remove('tic')\n",
    "        self.feature_names.remove('price')\n",
    "\n",
    "        self.features = {}\n",
    "        self.means = {}\n",
    "        self.stds = {}\n",
    "        self.prices = {}\n",
    "        for key, value in self.data_file.items():\n",
    "\n",
    "            self.means[key] = np.mean(value[self.feature_names], axis = 0)\n",
    "            self.stds[key] = np.std(value[self.feature_names], axis = 0)\n",
    "            \n",
    "            #Normalize features to have zero mean and unit standard deviation\n",
    "            self.features[key] = np.divide(value[self.feature_names] - self.means[key],\n",
    "                                          self.stds[key])\n",
    "            \n",
    "            self.prices[key] = value['price'].values\n",
    "        \n",
    "        pickle.dump(self.means, open(top_dir + 'feature_means', 'wb'))\n",
    "        pickle.dump(self.stds, open(top_dir + 'feature_stds', 'wb'))\n",
    "        \n",
    "        \n",
    "        self.prices = pd.DataFrame.from_dict(self.prices)\n",
    "        \n",
    "        #self.prices is a dataframe with each ticker being a key \n",
    "        #and the corresponding series representing the stock prices\n",
    "        \n",
    "        #Will be used later for normalization\n",
    "        \n",
    "        perc_changes = np.divide(self.prices[1:], self.prices[:-1]).values\n",
    "        \n",
    "        self.perc_means = np.mean(perc_changes, axis = 0)\n",
    "        self.perc_stds = np.std(perc_changes, axis = 0)\n",
    "        \n",
    "        pickle.dump(self.perc_means, open(top_dir + 'perc_means', 'wb'))\n",
    "        pickle.dump(self.perc_stds, open(top_dir + 'perc_stds', 'wb'))\n",
    "        \n",
    "\n",
    "        self.episode_length = EPISODE_LENGTH #number of trading minutes in episode\n",
    "\n",
    "        self.num_past_states = NUM_PAST_STATES #number of past days that are used in state\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.tickers) + 1,))\n",
    "                                            \n",
    "\n",
    "        obs_length = len(self.tickers) #observation due to past stacked states\n",
    "        obs_length += 1 #balance\n",
    "        obs_length += len(self.tickers) #holdings\n",
    "        obs_length += len(self.tickers)*len(self.feature_names) #number of technical analysis features\n",
    "        obs_length += len(self.tickers)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(obs_length,))\n",
    "        \n",
    "        self.trade_pen_frac = .00002\n",
    "\n",
    "    def step(self, action_):\n",
    "\n",
    "        \n",
    "        #Apply softmax to RL output so that actions sum to 1\n",
    "\n",
    "        action = softmax(10*action_.numpy()[0])\n",
    "\n",
    "        #Liquidate past holdings\n",
    "        self.balance += np.sum(self.holdings)\n",
    "        \n",
    "        trade_vol = np.sum(np.abs(self.last_action - action[:len(self.tickers)])*self.balance)\n",
    "        trade_pen = self.trade_pen_frac*trade_vol\n",
    "\n",
    "\n",
    "        self.last_action = action[:len(self.tickers)]\n",
    "        \n",
    "        self.balance = self.balance - trade_pen\n",
    "        \n",
    "        #New Portfolio at end of day\n",
    "        self.holdings = self.balance*action[:-1]\n",
    "        self.balance = self.balance*action[-1]\n",
    "        \n",
    "\n",
    "        \n",
    "        #Step into next day\n",
    "        self.index += TRADE_FREQ\n",
    "        #Get stock prices at next day\n",
    "        stock_obs = self.get_stock_obs(self.index)\n",
    "        self.next_prices = stock_obs[-1]\n",
    "        \n",
    "        #Update value of current holdings\n",
    "        perc_change = np.divide(self.next_prices, self.curr_prices)\n",
    "        self.holdings = np.multiply(self.holdings, perc_change)\n",
    "\n",
    "        self.curr_prices = self.next_prices\n",
    "        \n",
    "        self.net_worth = self.balance + np.sum(self.holdings)\n",
    "        \n",
    "\n",
    "        rew = self.net_worth - self.last_net_worth # reward is the delta between last net worth and current net worth\n",
    "\n",
    "        self.last_net_worth = self.net_worth\n",
    "        self.steps += TRADE_FREQ\n",
    "        done = (self.net_worth <= 0) or (self.steps >= self.episode_length)\n",
    "\n",
    "        obs = self.get_obs(stock_obs, self.balance, self.holdings, self.index)\n",
    "        self.cum_rew += rew\n",
    "\n",
    "        return obs, rew, done, {}\n",
    "    \n",
    "    \n",
    "    def get_stock_obs(self, index):\n",
    "\n",
    "        \n",
    "        ret= self.prices[index - self.num_past_states:index][self.tickers].values #stack data\n",
    "        return ret\n",
    "\n",
    "    def get_obs(self, stock_obs, balance, holdings, index):\n",
    "        #Normalize stock prices for inclusion in observations\n",
    "        perc_changes = np.divide(stock_obs[1:], stock_obs[:-1])[-1:]\n",
    "        perc_norm = np.divide(perc_changes - self.perc_means,\n",
    "                             self.perc_stds).reshape(-1,)\n",
    "\n",
    "        \n",
    "        feature_vals = np.array([])\n",
    "        ix = index - 1\n",
    "        #Add in features at current timestep, for each ticker\n",
    "        for tic in self.tickers:\n",
    "            feature_vals = np.append(feature_vals, (self.features[tic].iloc[ix][self.feature_names].values))\n",
    "        \n",
    "        #Form observation and normalize balance and holdings\n",
    "#         balance_norm = (balance - 1000.0/(len(self.tickers) + 1))/50.0\n",
    "#         holdings_norm = (holdings - 1000.0/(len(self.tickers) + 1))/50.0\n",
    "        \n",
    "        net_worth = balance + np.sum(holdings)\n",
    "        net_worth_norm = (net_worth - 1000.0)\n",
    "        holding_frac_norm = (holdings/net_worth - .5)*20\n",
    "        act_norm = (self.last_action - .5)*20\n",
    "        return np.concatenate([perc_norm, [net_worth_norm], holding_frac_norm, act_norm, feature_vals])\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        data_idx = np.random.randint(0, 10)\n",
    "        self.data_file = pickle.load(open('fake_data/train_ta_{}.p'.format(data_idx), 'rb'))\n",
    "        \n",
    "        for key, df in self.data_file.items():\n",
    "            df['macd'] = np.divide(df['macd'], df['price'])\n",
    "            df['macds'] = np.divide(df['macds'], df['price'])\n",
    "\n",
    "\n",
    "            df['boll_ub'] = np.divide(df['boll_ub'], df['price'])\n",
    "            df['boll_lb'] = np.divide(df['boll_lb'], df['price'])\n",
    "\n",
    "            df['open_14_sma'] = np.divide(df['open_14_sma'], df['price'])\n",
    "\n",
    "        self.features = {}\n",
    "\n",
    "        self.prices = {}\n",
    "        for key, value in self.data_file.items():\n",
    " \n",
    "            #Normalize features to have zero mean and unit standard deviation\n",
    "            self.features[key] = np.divide(value[self.feature_names] - self.means[key],\n",
    "                                          self.stds[key])\n",
    "            \n",
    "            self.prices[key] = value['price'].values\n",
    "\n",
    "        self.prices = pd.DataFrame.from_dict(self.prices)\n",
    "        \n",
    "        \n",
    "        self.cum_rew = 0.0\n",
    "        self.steps = 0\n",
    "        self.index = np.random.randint(2*NUM_PAST_STATES, len(self.prices) - EPISODE_LENGTH - 10)\n",
    "\n",
    "        #self.init_prices = self.prices[self.index-1:self.index + EPISODE_LENGTH]\n",
    "        stock_obs = self.get_stock_obs(self.index)\n",
    "        self.holdings = np.zeros(len(self.tickers)) #holdings of each stock in number of shares\n",
    "        self.balance = INITIAL_BALANCE\n",
    "        self.last_net_worth = INITIAL_BALANCE\n",
    "        self.net_worth = INITIAL_BALANCE\n",
    "        \n",
    "        self.curr_prices = stock_obs[-1]\n",
    "        \n",
    "        self.last_action = np.zeros(len(self.tickers))\n",
    "        \n",
    "        \n",
    "\n",
    "        obs = self.get_obs(stock_obs, self.balance, self.holdings, self.index)\n",
    "        return obs  # reward, done, info can't be included\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4982e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce26b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def init_gru_params(gru):\n",
    "    for name, param in gru.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.orthogonal_(param)\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "        \n",
    "#         print(self.logstd.sum())\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        output = torch.distributions.Normal(mean, std)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, num_layers_1=1, num_layers_2 = 2, hidden_size=64, learn_std=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "\n",
    "        # TODO changed for cartpole\n",
    "        self.act_dim = act_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        #self.num_layers = num_layers\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        seq_1 = [nn.Linear(obs_dim, hidden_size), nn.Tanh()]\n",
    "        for i in range(num_layers_1-1):\n",
    "            seq_1.extend([nn.Linear(hidden_size, hidden_size), nn.Tanh()])\n",
    "\n",
    "        self.fwd_actor = nn.Sequential(*seq_1)\n",
    "        self.fwd_critic = nn.Sequential(*seq_1)\n",
    "        \n",
    "\n",
    "        mid_layer = int(hidden_size)\n",
    "\n",
    "        seq_2_act = [nn.Linear(hidden_size, mid_layer), nn.Tanh()]\n",
    "        seq_2_obs = [nn.Linear(hidden_size, mid_layer), nn.Tanh()]\n",
    "        for i in range(num_layers_2-2):\n",
    "            seq_2_act.extend([nn.Linear(mid_layer, mid_layer), nn.Tanh()])\n",
    "            seq_2_obs.extend([nn.Linear(mid_layer, mid_layer), nn.Tanh()])\n",
    "            \n",
    "        seq_2_act.extend([nn.Linear(mid_layer, self.act_dim), nn.Tanh()])\n",
    "        seq_2_obs.extend([nn.Linear(mid_layer, 1)])\n",
    "\n",
    "        self.actor = nn.Sequential(*seq_2_act)\n",
    "        self.critic = nn.Sequential(*seq_2_obs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dist = DiagonalGaussian(self.act_dim, learn_std=learn_std)\n",
    "\n",
    "        #self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h_a=None, rnn_h_c=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        #price_obs = obs[:,:self.num_tickers * self.time_horizon]\n",
    "        #other_obs = obs[:,self.num_tickers * self.time_horizon:]\n",
    "        price_obs = obs\n",
    "\n",
    "        obs_actor = self.fwd_actor(price_obs)\n",
    "        obs_critic = self.fwd_critic(price_obs)\n",
    "            \n",
    "    \n",
    "        \n",
    "        forward_actor = self.actor(obs_actor)\n",
    "        action_dist = self.dist.evaluate(forward_actor)\n",
    "        \n",
    "        forward_critic = self.critic(obs_critic)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h_a, rnn_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eaad70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = torch.FloatTensor(np.random.uniform(size = (10, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d398920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['JNJ', 'JPM', 'DIS', 'HD'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_env.data_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1d550f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anubhavguha/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n"
     ]
    }
   ],
   "source": [
    "trade_env = TradeEnv(tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065981ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RACModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, num_tickers, num_layers_1=1, num_layers_2 = 2, hidden_size=64, learn_std=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "\n",
    "        # TODO changed for cartpole\n",
    "        self.act_dim = act_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.num_tickers = num_tickers\n",
    "        self.num_layers_1 = num_layers_1\n",
    "\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        #self.num_layers = num_layers\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        self.gru_actor = nn.GRU(self.num_tickers, hidden_size, num_layers=num_layers_1, batch_first=True)\n",
    "        self.gru_critic = nn.GRU(self.num_tickers, hidden_size, num_layers=num_layers_1, batch_first=True)\n",
    "            \n",
    "        \n",
    "\n",
    "        mid_layer = int(hidden_size)\n",
    "\n",
    "        seq_2_act = [nn.Linear(hidden_size, mid_layer), nn.Tanh()]\n",
    "        seq_2_obs = [nn.Linear(hidden_size, mid_layer), nn.Tanh()]\n",
    "        for i in range(num_layers_2-2):\n",
    "            seq_2_act.extend([nn.Linear(mid_layer, mid_layer), nn.Tanh()])\n",
    "            seq_2_obs.extend([nn.Linear(mid_layer, mid_layer), nn.Tanh()])\n",
    "            \n",
    "        seq_2_act.extend([nn.Linear(mid_layer, self.act_dim), nn.Tanh()])\n",
    "        seq_2_obs.extend([nn.Linear(mid_layer, 1)])\n",
    "\n",
    "        self.actor = nn.Sequential(*seq_2_act)\n",
    "        self.critic = nn.Sequential(*seq_2_obs)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dist = DiagonalGaussian(self.act_dim, learn_std=learn_std)\n",
    "\n",
    "        #self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(self.num_layers_1, batch_size, self.hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h_a=None, rnn_h_c=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        \n",
    "        price_obs = obs[:,:self.num_tickers]\n",
    "        other_obs = obs[:,self.num_tickers:]\n",
    "        \n",
    "\n",
    "        if rnn_h_a is None:\n",
    "            rnn_h_a = self.init_hidden(obs.size(0))\n",
    "        if rnn_h_c is None:\n",
    "            rnn_h_c = self.init_hidden(obs.size(0))\n",
    "\n",
    "        price_obs = torch.reshape(price_obs, (-1, self.num_tickers))\n",
    "        \n",
    "        \n",
    "        gru_obs = price_obs.unsqueeze(1)\n",
    "        \n",
    "\n",
    "\n",
    "        obs_actor, rnn_h_a = self.gru_actor(gru_obs, rnn_h_a)\n",
    "\n",
    "        obs_critic, rnn_h_c = self.gru_critic(gru_obs, rnn_h_c)\n",
    "\n",
    "        obs_actor = torch.squeeze(obs_actor, 1)\n",
    "        obs_critic = torch.squeeze(obs_critic, 1)\n",
    "        forward_actor = self.actor(obs_actor)\n",
    "        action_dist = self.dist.evaluate(forward_actor)\n",
    "        \n",
    "        forward_critic = self.critic(obs_critic)\n",
    "\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h_a, rnn_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4998ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.9998, gae_lambda=0.95, device=None):\n",
    "        # TODO changed for cartpole\n",
    "        self.episode_length = env.episode_length\n",
    "        self.env = env\n",
    "#         self.episode_length = 200 - 1\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        self.returns = None\n",
    "        \n",
    "        self.num_rollouts = 40\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros((self.num_rollouts*self.episode_length, len(self.env.tickers)+1), device=self.device)\n",
    "        self.values = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        self.returns = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        \n",
    "        self.log_probs = torch.zeros((self.num_rollouts*self.episode_length, len(self.env.tickers)+1), device=self.device)\n",
    "        self.obss = [None] * (self.episode_length*self.num_rollouts)\n",
    "        \n",
    "        self.gaes = torch.zeros(self.num_rollouts*self.episode_length, device=self.device)\n",
    "        \n",
    "        self.hidden_as = torch.zeros((self.num_rollouts*self.episode_length, 1, 1, 64), device=self.device)\n",
    "        self.hidden_cs = torch.zeros((self.num_rollouts*self.episode_length, 1, 1, 64), device=self.device)\n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        if isinstance(obs, list):\n",
    "            obs = np.stack(obs)\n",
    "\n",
    "        if len(obs.shape) == 1: # 1 dimensional\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            \n",
    "        return torch.FloatTensor(obs)\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        \n",
    "        total_return = 0\n",
    "        self.returns = []\n",
    "        \n",
    "        for ep in range(self.num_rollouts):\n",
    "            obs = env.reset()\n",
    "            hidden_a, hidden_c = None, None\n",
    "            \n",
    "            self.actions_ = torch.zeros((self.episode_length, len(self.env.tickers)+1), device=self.device)\n",
    "            self.values_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            self.rewards_ = torch.zeros(self.episode_length, device=self.device)\n",
    "            self.obss_ = [None] * (self.episode_length)\n",
    "            self.log_probs_ = torch.zeros((self.episode_length, len(self.env.tickers)+1), device=self.device)\n",
    "        \n",
    "            self.hidden_as_ = torch.zeros((self.episode_length, 1, 1, 64), device=self.device)\n",
    "            self.hidden_cs_ = torch.zeros((self.episode_length, 1, 1, 64), device=self.device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            T = 0\n",
    "        \n",
    "            while True:\n",
    "                with torch.no_grad():\n",
    "                    dist, value, hidden_a, hidden_c = self.acmodel(self.process_obs(obs),\n",
    "                                                                  rnn_h_a = hidden_a,\n",
    "                                                                  rnn_h_c = hidden_c)\n",
    "\n",
    "                action = dist.sample()\n",
    "\n",
    "                self.obss_[T] = obs\n",
    "\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                \n",
    "                total_return += reward\n",
    "                \n",
    "                self.actions_[T] = action[0]\n",
    "                self.values_[T] = value\n",
    "                self.rewards_[T] = float(reward)\n",
    "                self.log_probs_[T] = dist.log_prob(action)[0]\n",
    "\n",
    "                self.hidden_as_[T] = hidden_a\n",
    "                self.hidden_cs_[T] = hidden_c\n",
    "\n",
    "\n",
    "                T += 1\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            self.actions[ep*self.episode_length:(ep+1)*self.episode_length] = self.actions_[:T]\n",
    "            self.values[ep*self.episode_length:(ep+1)*self.episode_length] = self.values_[:T]\n",
    "            self.rewards[ep*self.episode_length:(ep+1)*self.episode_length] = self.rewards_[:T]\n",
    "            \n",
    "            self.hidden_as[ep*self.episode_length:(ep+1)*self.episode_length] = self.hidden_as_[:T]\n",
    "            self.hidden_cs[ep*self.episode_length:(ep+1)*self.episode_length] = self.hidden_cs_[:T]\n",
    "            \n",
    "            \n",
    "            discounted_reward = 0.0\n",
    "            self.returns_ = []\n",
    "            for r in reversed(self.rewards_):\n",
    "                discounted_reward = r + self.discount*discounted_reward\n",
    "                self.returns_.insert(0, discounted_reward)\n",
    "            self.returns[ep*self.episode_length:(ep+1)*self.episode_length] = self.returns_[:T]\n",
    "            \n",
    "\n",
    "            self.log_probs[ep*self.episode_length:(ep+1)*self.episode_length] = self.log_probs_[:T]\n",
    "            \n",
    "            self.obss[ep*self.episode_length:(ep+1)*self.episode_length] = self.process_obs(self.obss_[:T])\n",
    "            self.gaes_ = self.compute_advantage_gae(self.rewards_, self.values_, T)\n",
    "            self.gaes[ep*self.episode_length:(ep+1)*self.episode_length] = self.gaes_[:T]\n",
    "            \n",
    "        self.obss = torch.FloatTensor(np.stack(self.obss))\n",
    "        self.returns = torch.FloatTensor(self.returns)\n",
    "        \n",
    "        return total_return/self.num_rollouts, T\n",
    "            \n",
    "    def compute_advantage_gae(self, rewards, values, T):\n",
    "\n",
    "        deltas = torch.cat((rewards[:-1] + self.discount*values[1:] - values[:-1], rewards[-1:] - values[-1:]))\n",
    "        deltas_flip = torch.flip(deltas, [0])\n",
    "        A = 0.0*deltas_flip[0]\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in range(len(deltas_flip)):\n",
    "            A = self.discount*self.gae_lambda*A + deltas_flip[i]\n",
    "            advantages[i] = A\n",
    "        advantages = torch.flip(advantages, [0])\n",
    "        return advantages\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e2fb878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=2e-4,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=10):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "\n",
    "        dist, _, _, _ = self.acmodel(rollouts.obss) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions.view(-1,self.acmodel.act_dim)).detach()\n",
    "        \n",
    "\n",
    "#         policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "#         value_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "        \n",
    "        batch_size = 256\n",
    "        avg_policy_loss, avg_value_loss = 0, 0\n",
    "        for i in range(self.train_iters):\n",
    "            rand_idxs = np.random.choice(np.arange(len(rollouts.obss)), batch_size , replace = False)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss[rand_idxs], old_logp[rand_idxs], \n",
    "                                                               rollouts.actions[rand_idxs], rollouts.gaes[rand_idxs],\n",
    "                                                              rollouts.hidden_as[rand_idxs],\n",
    "                                                              rollouts.hidden_cs[rand_idxs])\n",
    "\n",
    "            v_loss = self._compute_value_loss(rollouts.obss[rand_idxs], rollouts.returns[rand_idxs],\n",
    "                                             rollouts.hidden_as[rand_idxs],\n",
    "                                                              rollouts.hidden_cs[rand_idxs])\n",
    "            #print ('vloss:', v_loss, 'pi_loss:', pi_loss)\n",
    "            loss = .01*v_loss + pi_loss\n",
    "            \n",
    "            avg_policy_loss += pi_loss.item()\n",
    "            avg_value_loss += v_loss.item()\n",
    "   \n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "            \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "            #loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(acmodel.parameters(), .5)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return avg_policy_loss/self.train_iters, avg_value_loss/self.train_iters\n",
    "        \n",
    "    def _compute_policy_loss_ppo(self, obs, old_logp, actions, advantages, hidden_as, hidden_cs):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "        \n",
    "\n",
    "        dist, _, _, _ = self.acmodel(obs, \n",
    "                                     rnn_h_a = torch.swapaxes(torch.squeeze(hidden_as, 2), 0, 1),\n",
    "                                    rnn_h_c = torch.swapaxes(torch.squeeze(hidden_cs, 2), 0, 1))\n",
    "        \n",
    "        new_logp = dist.log_prob(actions.view(-1,self.acmodel.act_dim))\n",
    "        \n",
    "        entropy = torch.mean(dist.entropy())\n",
    "        entropy = entropy * self.entropy_coef\n",
    "\n",
    "\n",
    "        r = torch.exp(new_logp - old_logp.detach())\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-self.clip_ratio, 1+self.clip_ratio)*advantages.view(-1,1)\n",
    "        \n",
    "        min_advs = torch.minimum(r*advantages.view(-1,1), clamp_adv)\n",
    "        \n",
    "\n",
    "        policy_loss = -torch.mean(min_advs) - entropy\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns, hidden_as, hidden_cs):\n",
    "        _, values, _, _ = self.acmodel(obs, \n",
    "                                       rnn_h_a = torch.swapaxes(torch.squeeze(hidden_as, 2), 0, 1),\n",
    "                                    rnn_h_c = torch.swapaxes(torch.squeeze(hidden_cs, 2), 0, 1))\n",
    "        \n",
    "        value_loss = torch.mean((returns.view(-1,1) - values.view(-1,1))**2)\n",
    "\n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86599a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 6.884 HW4\n",
    "\n",
    "def run_experiment(acmodel, env, ppo_kwargs, rollout_kwargs = {}, max_episodes=200000, score_threshold=0.8):\n",
    "    # acmodel_args should be dictionary corresponding to inputs of acmodel\n",
    "    # ie {num_tickers: 4, time_horizon: 5, etc..}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "    \n",
    "    SMOOTH_REWARD_WINDOW = 30\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "    \n",
    "    num_frames = 0\n",
    "    rollouts = RolloutBuffer(acmodel, env, **rollout_kwargs)\n",
    "    ppo = PPO(acmodel, **ppo_kwargs)\n",
    "\n",
    "    pbar = tqdm(range(max_episodes))\n",
    "    for update in pbar:\n",
    "        rollouts.reset() # resetting the buffer\n",
    "        total_return, T = rollouts.collect_experience()\n",
    "        policy_loss, value_loss = ppo.update(rollouts)\n",
    "        \n",
    "        num_frames += T\n",
    "        rewards.append(total_return)\n",
    "        \n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':total_return, 'policy_loss': policy_loss, 'value_loss': value_loss}\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "#         if update % 20 == 0 and update != 0:\n",
    "#             pickle.dump(pd.DataFrame(pd_logs).set_index('episode'),\n",
    "#                        open('train_df.p', 'wb'))\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "    else:\n",
    "        print('Unsolved. Check your implementation.')\n",
    "    \n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9fc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TradeEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c4b9689c10b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTradeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TradeEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = TradeEnv(tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7062cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 754/70000 [1:03:00<96:27:18,  5.01s/it, episode=753, num_frames=22620, smooth_reward=4.78, reward=10, policy_loss=-1.76, value_loss=3.51e+3]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1c4aa4866a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   **acmodel_kwargs)\n\u001b[1;32m      8\u001b[0m \u001b[0mppo_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entropy_coef'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_iters'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfwd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-0d6b17e81d9e>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(acmodel, env, ppo_kwargs, rollout_kwargs, max_episodes, score_threshold)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# resetting the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtotal_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-c30b3a2e3d23>\u001b[0m in \u001b[0;36mcollect_experience\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobss_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mtotal_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-02e579cdf1b9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_worth\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholdings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_rew\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-02e579cdf1b9>\u001b[0m in \u001b[0;36mget_obs\u001b[0;34m(self, stock_obs, balance, holdings, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#Add in features at current timestep, for each ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mfeature_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#Form observation and normalize balance and holdings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2953\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2955\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2956\u001b[0m             )\n\u001b[1;32m   2957\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# we will try to copy be-definition here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;31m# perf shortcut as this is the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmaybe_castable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_castable\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_timedelta64_ns_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPOSSIBLY_CAST_DTYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_ind = len(TECHNICAL_INDICATORS_LIST) + 1 # plus volume\n",
    "\n",
    "acmodel_kwargs = {'hidden_size': 64, 'num_layers_1': 1, 'num_layers_2': 2}\n",
    "acmodel = RACModel(env.observation_space.shape[0],\n",
    "                  env.action_space.shape[0],\n",
    "                   len(tickers),\n",
    "                  **acmodel_kwargs)\n",
    "ppo_kwargs = {'lr': 1e-3, 'entropy_coef': 0.01, 'train_iters': 10}\n",
    "fwd_df = run_experiment(acmodel, env, ppo_kwargs, max_episodes=70000, score_threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd0d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(acmodel.state_dict(), 'rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "83bddcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 577/70000 [33:59<68:09:07,  3.53s/it, episode=576, num_frames=17310, smooth_reward=24.9, reward=42.4, policy_loss=-10.2, value_loss=2.96e+3]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-7717ea8e1b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   **acmodel_kwargs)\n\u001b[1;32m      6\u001b[0m \u001b[0mppo_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entropy_coef'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_iters'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfwd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-0d6b17e81d9e>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(acmodel, env, ppo_kwargs, rollout_kwargs, max_episodes, score_threshold)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# resetting the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtotal_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-bb6bcc1825a5>\u001b[0m in \u001b[0;36mcollect_experience\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-6006f751a330>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, rnn_h_a, rnn_h_c)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprice_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mobs_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mobs_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_backward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_ind = len(TECHNICAL_INDICATORS_LIST) + 1 # plus volume\n",
    "acmodel_kwargs = {'hidden_size': 128, 'num_layers_1': 1, 'num_layers_2': 2}\n",
    "acmodel = ACModel(env.observation_space.shape[0],\n",
    "                  env.action_space.shape[0],\n",
    "                  **acmodel_kwargs)\n",
    "ppo_kwargs = {'lr': 2e-3, 'entropy_coef': 0.01, 'train_iters': 10}\n",
    "fwd_df = run_experiment(acmodel, env, ppo_kwargs, max_episodes=70000, score_threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30e301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5aebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f2b98405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc021c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
