{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "modified-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "seeing-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_tickers, time_horizon, num_ta_indicators, recurrent, batch_size=1, hidden_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        self.num_tickers = num_tickers\n",
    "        self.time_horizon = time_horizon\n",
    "        self.num_ta_indicators = num_ta_indicators\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # TODO changed for cartpole\n",
    "        # action_dim = num_tickers + 1 # buy/sell for each ticker + 1 for cash\n",
    "        action_dim = 1\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            self.hidden = self.init_hidden(batch_size, hidden_size)\n",
    "            num_inputs = num_tickers\n",
    "            self.gru = nn.GRU(num_inputs, hidden_size, batch_first=True) # (batch, seq, feature)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "        else:\n",
    "            num_inputs = num_tickers * time_horizon\n",
    "            self.fwd = nn.Sequential(nn.Linear(num_inputs, hidden_size), nn.Tanh())\n",
    "        \n",
    "        # output of gru/fwd, cash, holdings, and then all the ta indicators\n",
    "        # TODO changed for cartpole\n",
    "        # num_inputs = hidden_size + 1 + num_tickers + num_tickers * num_ta_indicators\n",
    "        num_inputs = hidden_size\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.dist = DiagonalGaussian(action_dim)\n",
    "\n",
    "        self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.FloatTensor(obs)\n",
    "        price_obs = obs[:,:self.num_tickers * self.time_horizon]\n",
    "        other_obs = obs[:,self.num_tickers * self.time_horizon:]\n",
    "        \n",
    "        if self.recurrent:\n",
    "            if rnn_h is None:\n",
    "                rnn_h = self.hidden\n",
    "            # TODO figure out dimensionality.. I tried my best but probs still bugs here\n",
    "            obs = torch.reshape(price_obs, (self.batch_size, self.time_horizon, -1))\n",
    "            obs, rnn_h = self.gru(obs, rnn_h)\n",
    "            obs = obs[:,-1,:] # selecting the last element\n",
    "        else:\n",
    "            obs = self.fwd(price_obs)\n",
    "            \n",
    "        obs = torch.cat((obs, other_obs), 1)\n",
    "\n",
    "        action_dist = self.dist.evaluate(self.actor(obs))\n",
    "        \n",
    "        forward_critic = self.critic(obs)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "demographic-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.FloatTensor(\n",
    "    [[1, 2, 3, 4, 5, 6, 100, 0, 0],\n",
    "    [2, 1, 4, 3, 6, 5, 50, 25, 25]]\n",
    ")\n",
    "acmodel = ACModel(num_tickers=2, time_horizon=3, num_ta_indicators=0, recurrent=True, batch_size=2, hidden_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "noble-insert",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist, value, _ = acmodel(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "brave-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6301, -2.3705, -0.6638],\n",
       "        [ 0.3332,  0.0860,  2.5512]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "fuzzy-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.995, gae_lambda=0.95, device=None):\n",
    "        # TODO changed for cartpole\n",
    "        # self.episode_length = env.episode_length\n",
    "        self.episode_length = 200\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.values = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.log_probs = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.obss = [None] * self.episode_length\n",
    "        \n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        if isinstance(obs, list):\n",
    "            obs = np.stack(obs)\n",
    "        elif len(obs.shape) == 1: # 1 dimensional\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "        return obs\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        obs = env.reset()\n",
    "        total_return = 0\n",
    "        T = 0\n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                dist, value, _ = self.acmodel(self.process_obs(obs))\n",
    "                \n",
    "            action = dist.sample()\n",
    "            \n",
    "            self.obss[T] = obs\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_return += reward\n",
    "            \n",
    "            self.actions[T] = action\n",
    "            self.values[T] = value\n",
    "            self.rewards[T] = float(reward)\n",
    "            self.log_probs[T] = dist.log_prob(action)\n",
    "            \n",
    "            \n",
    "            T += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        self.actions = self.actions[:T]\n",
    "        self.values = self.values[:T]\n",
    "        self.rewards = self.rewards[:T]\n",
    "        self.log_probs = self.log_probs[:T]\n",
    "        self.gaes = self.compute_advantage_gae(T)\n",
    "            \n",
    "    def compute_advantage_gae(self, T):\n",
    "        def _delta(t, rewards, discount, values):\n",
    "            return rewards[t] + ((discount * values[t+1] - values[t]) if t+1 < values.shape[0] else 0)\n",
    "\n",
    "        advantages = torch.zeros_like(self.values)\n",
    "\n",
    "        n = self.values.shape[0]\n",
    "        for t in range(n):\n",
    "            advantages[t] = sum([(self.gae_lambda*self.discount)**i * _delta(t+i, self.rewards, self.discount, self.values) for i in range(n-t)])\n",
    "\n",
    "        return advantages[:T]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "satisfactory-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=1e-3,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=5):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "        dist, _, _ = self.acmodel(rollouts.process_obs(rollouts.obss)) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions)\n",
    "        \n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "        value_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "        \n",
    "        for i in range(self.train_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "            v_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "            \n",
    "            loss = v_loss + pi_loss\n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "                \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "        \n",
    "    def _compute_policy_loss_ppo(obs, old_logp, actions, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _, _ = self.acmodel(obs)\n",
    "        new_logp = dist.log_prob(actions)\n",
    "\n",
    "        new_p, old_p = torch.exp(new_logp), torch.exp(old_logp)\n",
    "        r = new_p / old_p\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-args.clip_ratio, 1+args.clip_ratio)*advantages\n",
    "        min_advs = torch.minimum(r*advantages, torch.clamp(r, 1-args.clip_ratio, 1+args.clip_ratio)*advantages)\n",
    "\n",
    "        policy_loss = -torch.mean(min_advs)\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(obs, returns):\n",
    "        _, values = acmodel(obs)\n",
    "        value_loss = torch.mean((returns - values)**2)\n",
    "\n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "temporal-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config params from the pset\n",
    "\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=False,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=False):\n",
    "        \n",
    "        self.score_threshold = score_threshold\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.log_interval = log_interval\n",
    "        self.max_episodes = max_episodes\n",
    "        self.use_critic = use_critic\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.target_kl = target_kl\n",
    "        self.train_ac_iters = train_ac_iters\n",
    "        self.gae_lambda=gae_lambda\n",
    "        self.use_discounted_reward=use_discounted_reward\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.use_gae = use_gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "outside-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import registry, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "adult-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "        \n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "    \n",
    "    def _qdotdot(self, q, u):\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "        \n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "        \n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) - \n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "                \n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "    \n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "    \n",
    "    def step(self, q, u):\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)    \n",
    "            \n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "            \n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "            \n",
    "        return new_q\n",
    "    \n",
    "    # given q [n, q_shape] and u [n, t] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        qs = [q]\n",
    "        \n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "                \n",
    "        return torch.stack(qs, dim=1)\n",
    "    \n",
    "    def reward(self, q, u):\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "                        \n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "    \n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, time_horizon=5, timestep_limit=200):\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "        \n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.time_horizon = time_horizon\n",
    "#         self.memory = np.zeros(4 * time_horizon)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "        \n",
    "        for t in range(self.time_horizon-1):\n",
    "            # start with a few random actions to initialize\n",
    "            action = np.random.uniform(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "            self.step(action)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.q_sim\n",
    "            \n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "        \n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "        \n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "        \n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "        \n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "        \n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten(), reward, done, {}\n",
    "    \n",
    "    def is_done(self):\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "    \n",
    "env_name = 'CartpoleWithMemory-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n",
    "env = gym.make('CartpoleWithMemory-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "incomplete-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.25640482e-04, -5.42734137e-04,  1.62820241e-02, -2.71367069e-02,\n",
       "        6.89450164e-03, -1.14872851e-02,  3.28443058e-01, -5.47227546e-01,\n",
       "        1.34394828e-02, -2.23169134e-02,  3.27249059e-01, -5.41481419e-01,\n",
       "        1.11184776e-02, -1.82276369e-02, -1.16050262e-01,  2.04463829e-01])"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "excessive-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "acmodel = ACModel(num_tickers=4, time_horizon=5, num_ta_indicators=0, recurrent=True, batch_size=1, hidden_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "comparable-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "spatial-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(acmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "assumed-lancaster",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-558-58e68a1a944d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-550-bfdf3580dae9>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# rollouts should be RolloutBuffer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO may need to process these observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mold_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-549-8d497c172066>\u001b[0m in \u001b[0;36mprocess_obs\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# TODO: formatting stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 1 dimensional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/6.884/finrl/venv/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-force",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl2",
   "language": "python",
   "name": "finrl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
