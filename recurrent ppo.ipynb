{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "modified-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-classroom",
   "metadata": {},
   "source": [
    "# Architecture for ACModel, Rollouts, and PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "seeing-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "        \n",
    "#         print(self.logstd.sum())\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_tickers, time_horizon, num_ta_indicators, recurrent, hidden_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        self.num_tickers = num_tickers\n",
    "        self.time_horizon = time_horizon\n",
    "        self.num_ta_indicators = num_ta_indicators\n",
    "        \n",
    "        # TODO changed for cartpole\n",
    "        # action_dim = num_tickers + 1 # buy/sell for each ticker + 1 for cash\n",
    "        action_dim = 1\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            self.hidden = self.init_hidden(time_horizon, hidden_size)\n",
    "            num_inputs = num_tickers\n",
    "            self.gru = nn.GRU(num_inputs, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "        else:\n",
    "            num_inputs = num_tickers * time_horizon\n",
    "            self.fwd = nn.Sequential(nn.Linear(num_inputs, hidden_size), nn.Tanh())\n",
    "        \n",
    "        # output of gru/fwd, cash, holdings, and then all the ta indicators\n",
    "        # TODO changed for cartpole\n",
    "        # num_inputs = hidden_size + 1 + num_tickers + num_tickers * num_ta_indicators\n",
    "        num_inputs = hidden_size\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.dist = DiagonalGaussian(action_dim, learn_std=False)\n",
    "\n",
    "        self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        price_obs = obs[:,:self.num_tickers * self.time_horizon]\n",
    "        other_obs = obs[:,self.num_tickers * self.time_horizon:]\n",
    "        \n",
    "        if self.recurrent:\n",
    "            if rnn_h is None:\n",
    "                rnn_h = self.hidden\n",
    "            \n",
    "            obs = torch.reshape(price_obs, (-1, self.time_horizon, self.num_tickers))\n",
    "            obs, rnn_h = self.gru(obs, rnn_h)\n",
    "            obs = obs[:,-1,:] # selecting the last element\n",
    "        else:\n",
    "            obs = self.fwd(price_obs)\n",
    "            \n",
    "        obs = torch.cat((obs, other_obs), 1)\n",
    "\n",
    "        forward_actor = self.actor(obs)\n",
    "        action_dist = self.dist.evaluate(forward_actor)\n",
    "        \n",
    "        forward_critic = self.critic(obs)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "fuzzy-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.995, gae_lambda=0.95, device=None):\n",
    "        # TODO changed for cartpole\n",
    "        # self.episode_length = env.episode_length\n",
    "        self.episode_length = 200\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.values = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.log_probs = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.obss = [None] * self.episode_length\n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        if isinstance(obs, list):\n",
    "            obs = np.stack(obs)\n",
    "\n",
    "        if len(obs.shape) == 1: # 1 dimensional\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            \n",
    "        return torch.FloatTensor(obs)\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        obs = env.reset()\n",
    "        total_return = 0\n",
    "        T = 0\n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                dist, value, _ = self.acmodel(self.process_obs(obs))\n",
    "                \n",
    "            action = dist.sample()\n",
    "            \n",
    "            self.obss[T] = obs\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_return += reward\n",
    "            \n",
    "            self.actions[T] = action\n",
    "            self.values[T] = value\n",
    "            self.rewards[T] = float(reward)\n",
    "            self.log_probs[T] = dist.log_prob(action)\n",
    "            \n",
    "            \n",
    "            T += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        self.actions = self.actions[:T]\n",
    "        self.values = self.values[:T]\n",
    "        self.rewards = self.rewards[:T]\n",
    "        self.log_probs = self.log_probs[:T]\n",
    "        self.obss = self.process_obs(self.obss[:T])\n",
    "        self.gaes = self.compute_advantage_gae(T)\n",
    "        \n",
    "        return total_return, T\n",
    "            \n",
    "    def compute_advantage_gae(self, T):\n",
    "        def _delta(t, rewards, discount, values):\n",
    "            return rewards[t] + ((discount * values[t+1] - values[t]) if t+1 < values.shape[0] else 0)\n",
    "\n",
    "        advantages = torch.zeros_like(self.values)\n",
    "\n",
    "        n = self.values.shape[0]\n",
    "        for t in range(n):\n",
    "            advantages[t] = sum([(self.gae_lambda*self.discount)**i * _delta(t+i, self.rewards, self.discount, self.values) for i in range(n-t)])\n",
    "\n",
    "        return advantages[:T]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=1e-3,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=5):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "        dist, _, _ = self.acmodel(rollouts.obss) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions).detach()\n",
    "        \n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "        value_loss = self._compute_value_loss(rollouts.obss, rollouts.rewards)\n",
    "        \n",
    "        for i in range(self.train_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "            v_loss = self._compute_value_loss(rollouts.obss, rollouts.rewards)\n",
    "            \n",
    "            loss = v_loss + pi_loss\n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "            \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "        \n",
    "    def _compute_policy_loss_ppo(self, obs, old_logp, actions, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _, _ = self.acmodel(obs)\n",
    "#         dist = self.acmodel.dist.evaluate(a)\n",
    "        new_logp = dist.log_prob(actions)\n",
    "    \n",
    "        print('new logp', new_logp)\n",
    "        print('old logp', old_logp)\n",
    "\n",
    "        new_p, old_p = torch.exp(new_logp), torch.exp(old_logp)\n",
    "        r = new_p / old_p\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-self.clip_ratio, 1+self.clip_ratio)*advantages\n",
    "        min_advs = torch.minimum(r*advantages, torch.clamp(r, 1-self.clip_ratio, 1+self.clip_ratio)*advantages)\n",
    "\n",
    "        policy_loss = -torch.mean(min_advs)\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, values, _ = self.acmodel(obs)\n",
    "        value_loss = torch.mean((returns - values)**2)\n",
    "\n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-engagement",
   "metadata": {},
   "source": [
    "# Cartpole Gym Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "outside-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import registry, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "adult-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "        \n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "    \n",
    "    def _qdotdot(self, q, u):\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "        \n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "        \n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) - \n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "                \n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "    \n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "    \n",
    "    def step(self, q, u):\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)    \n",
    "            \n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "            \n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "            \n",
    "        return new_q\n",
    "    \n",
    "    # given q [n, q_shape] and u [n, t] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        qs = [q]\n",
    "        \n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "                \n",
    "        return torch.stack(qs, dim=1)\n",
    "    \n",
    "    def reward(self, q, u):\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "                        \n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "    \n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, time_horizon=5, timestep_limit=200):\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "        \n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.time_horizon = time_horizon\n",
    "#         self.memory = np.zeros(4 * time_horizon)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "        \n",
    "        for t in range(self.time_horizon-1):\n",
    "            # start with a few random actions to initialize\n",
    "            action = np.random.uniform(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "            self.step(action)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.q_sim\n",
    "            \n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "        \n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "        \n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "        \n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "        \n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "        \n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten(), reward, done, {}\n",
    "    \n",
    "    def is_done(self):\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "    \n",
    "env_name = 'CartpoleWithMemory-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n",
    "env = gym.make('CartpoleWithMemory-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "incomplete-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.00367199,\n",
       "       -0.00611999,  0.18359971, -0.30599952,  0.00358292, -0.00593162,\n",
       "       -0.00445383,  0.00941841,  0.00464291, -0.00761954,  0.0529995 ,\n",
       "       -0.08439583,  0.01323795, -0.02181568,  0.42975213, -0.70980709])"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "excessive-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225.5750732421875, 13993.33984375)"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recurrent model\n",
    "acmodel = ACModel(num_tickers=4, time_horizon=5, num_ta_indicators=0, recurrent=True, hidden_size=32)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "chief-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2285.906494140625, 42791.0390625)"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feedforward model\n",
    "acmodel = ACModel(num_tickers=4, time_horizon=5, num_ta_indicators=0, recurrent=False, hidden_size=32)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-bottle",
   "metadata": {},
   "source": [
    "Ok so looks like the Cartpole gym syntactically works.. Let's try to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-optimum",
   "metadata": {},
   "source": [
    "# Run experiment on Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "selected-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 6.884 HW4\n",
    "\n",
    "def run_experiment(acmodel_args, max_episodes=200000, score_threshold=0.8):\n",
    "    # acmodel_args should be dictionary corresponding to inputs of acmodel\n",
    "    # ie {num_tickers: 4, time_horizon: 5, etc..}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make('CartpoleWithMemory-v0')\n",
    "    \n",
    "    acmodel = ACModel(num_tickers=acmodel_args['num_tickers'], time_horizon=acmodel_args['time_horizon'], \n",
    "                      num_ta_indicators=acmodel_args['num_ta_indicators'], recurrent=acmodel_args['recurrent'], \n",
    "                      hidden_size=acmodel_args.get('hidden_size', 32))\n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "    \n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "    \n",
    "    num_frames = 0\n",
    "    rollouts = RolloutBuffer(acmodel, env)\n",
    "    ppo = PPO(acmodel)\n",
    "\n",
    "    pbar = tqdm(range(max_episodes))\n",
    "    for update in pbar:\n",
    "        rollouts.reset() # resetting the buffer\n",
    "        total_return, T = rollouts.collect_experience()\n",
    "        \n",
    "        policy_loss, value_loss = ppo.update(rollouts)\n",
    "        num_frames += T\n",
    "        rewards.append(total_return)\n",
    "        \n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':total_return, 'policy_loss': policy_loss}\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "    else:\n",
    "        print('Unsolved. Check your implementation.')\n",
    "    \n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "complimentary-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 261/200000 [03:23<59:42:24,  1.08s/it, episode=260, num_frames=51417, smooth_reward=-6.23, reward=-25.4, policy_loss=2.35]    /home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in ExpBackward. Traceback of forward call that caused the error:\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2894, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3165, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-711-e35ded84c9c5>\", line 2, in <module>\n",
      "    df = run_experiment(acmodel_args, score_threshold=0)\n",
      "  File \"<ipython-input-710-24b41a624c9f>\", line 29, in run_experiment\n",
      "    policy_loss, value_loss = ppo.update(rollouts)\n",
      "  File \"<ipython-input-703-6e3c6115d1dd>\", line 28, in update\n",
      "    pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
      "  File \"<ipython-input-703-6e3c6115d1dd>\", line 49, in _compute_policy_loss_ppo\n",
      "    new_p, old_p = torch.exp(new_logp), torch.exp(old_logp)\n",
      " (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "  0%|          | 261/200000 [03:24<43:30:23,  1.28it/s, episode=260, num_frames=51417, smooth_reward=-6.23, reward=-25.4, policy_loss=2.35]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'ExpBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-711-e35ded84c9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_tickers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_horizon'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_ta_indicators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recurrent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-710-24b41a624c9f>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(acmodel_args, max_episodes, score_threshold)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-703-6e3c6115d1dd>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lol todo are we supposed to retain graph?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/6.884/finrl/venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/6.884/finrl/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'ExpBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "# feedforward\n",
    "acmodel_args = {'num_tickers': 4, 'time_horizon': 5, 'num_ta_indicators': 0, 'recurrent': False, 'hidden_size': 64}\n",
    "df = run_experiment(acmodel_args, score_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "final-plymouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='num_frames'>"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEHCAYAAABCwJb2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa+ElEQVR4nO3df3RU5b3v8feXkJIKCBiiqAThHFEEBIJRBM8gR1YtUn/U6lGpomhvUXtYba2lyDm2WluXFn90iVKV0yL+qsVSbi/2gOIPvKAHlIBBQLQGDZcAp3KgCCig4Pf+sXdwjJlkkuxJAs/ntVYWe/Y8+9nf2Zn5sLNn5nnM3RERkTC0aekCRESk+Sj0RUQCotAXEQmIQl9EJCAKfRGRgLRtqR137drVe/bs2VK7FxE5KC1fvvx/3L2osdu3WOj37NmTsrKyltq9iMhByczWN2V7Xd4REQlIvaFvZsVmttDM3jKzNWb2g1rajDCzD82sPP75WW7KFRGRpsjm8s4+4EZ3X2FmHYHlZva8u79Vo91idz83+RJFRCQp9Ya+u28GNsfLO81sLXAsUDP0RaQV+fTTT6mqqmLPnj0tXYo0QkFBAd27dyc/Pz/Rfhv0Rq6Z9QRKgNdquXuoma0ENgE/dvc1tWw/HhgP0KNHjwYXKyLZq6qqomPHjvTs2RMza+lypAHcna1bt1JVVUWvXr0S7TvrN3LNrAPwJ+CH7r6jxt0rgOPcfSBwP/Dn2vpw9+nuXurupUVFjf7EkYhkYc+ePRQWFirwD0JmRmFhYU7+Sssq9M0snyjwn3T3OTXvd/cd7r4rXp4H5JtZ10QrFZEGU+AfvHL1u8vm0zsG/A5Y6+73ZmjTLW6HmZ0W97s1yUJFRKTpsjnTPwMYC5yV9pHM0WZ2nZldF7e5GFgdX9OfClzmGqhfRFrQyy+/zLnn6gOFNWXz6Z1XgDr/znD3B4AHkipKRA497o6706ZNbr4Tun//fvLy8nLS96FE38gVkZyprKzkxBNP5Morr6R///784he/4NRTT2XAgAHccsstANx1111MnToVgBtuuIGzzjoLgJdeeonLL78cgOuvv57S0lL69et3YDuIhnOZNGkSgwcP5o9//CPPPvssffr0YfDgwcyZ86W3H4UWHHtHRJrPz59Zw1uban7ormn6HnM4t5zXr9527777Lo8++ig7duxg9uzZvP7667g7559/PosWLSKVSnHPPffw/e9/n7KyMvbu3cunn37K4sWLGT58OAC33347RxxxBPv372fkyJG8+eabDBgwAIDCwkJWrFjBnj176N27Ny+99BLHH388l156aaKP91ChM30RyanjjjuO008/nQULFrBgwQJKSkoYPHgwb7/9Nu+++y6nnHIKy5cvZ8eOHbRr146hQ4dSVlbG4sWLSaVSADz99NMMHjyYkpIS1qxZw1tvff7d0Opwf/vtt+nVqxe9e/fGzLjiiita5PG2djrTFwlANmfkudK+fXsguqY/efJkrr322i+16dWrFzNnzmTYsGEMGDCAhQsXUlFRwUknncT777/P3XffzbJly+jSpQvjxo37wufXq/uX7OhMX0Saxde//nVmzJjBrl27ANi4cSMffPABAKlUirvvvpvhw4eTSqV46KGHKCkpwczYsWMH7du3p1OnTvztb39j/vz5tfbfp08fKisrWbduHQBPPfVU8zywg4zO9EWkWZx99tmsXbuWoUOHAtChQweeeOIJjjzySFKpFLfffjtDhw6lffv2FBQUHLi0M3DgQEpKSujTpw/FxcWcccYZtfZfUFDA9OnT+cY3vsFhhx1GKpVi586dzfb4DhbWUh+nLy0tdU2iIpI7a9eu5aSTTmrpMqQJavsdmtlydy9tbJ+6vCMiEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLyEGvsrKS3//+9wduz5w5kwkTJrRgRdmrrKykf//+zbY/hb6IHPRqhn5T7N+/P5F+Mtm3b19O+6+PhmEQCcH8m+C/VyXbZ7eT4Zw762zy0Ucfcckll1BVVcX+/fv56U9/yqRJkxgzZgzz58+nbdu2TJ8+ncmTJ1NRUcHEiRO57rrrcHd+8pOfMH/+fMyMm2++mUsvvTTj+ptuuom1a9cyaNAgrrrqKrp06cKmTZsYNWoU69at48ILL2TKlCkZ6+zQoQPXXnstL7zwAtOmTaOyspKpU6fyySefMGTIEH7zm98wZ84clixZwr333st9993Hfffdx3vvvcd7773H2LFjefXVV7ntttt45pln2L17N8OGDePhhx/GzBgxYgSDBg3ilVdeYcyYMYwYMYJrrrkGiIanaE460xeRnHn22Wc55phjWLlyJatXr2bUqFEA9OjRg/LyclKpFOPGjWP27NksXbr0wAQpc+bMoby8nJUrV/LCCy8wceJENm/enHH9nXfeSSqVory8nBtuuAGA8vJyZs2axapVq5g1axYbNmzIWOdHH33EkCFDWLlyJYWFhcyaNYtXX32V8vJy8vLyePLJJ0mlUixevBiAxYsXU1hYyMaNG78w7v+ECRNYtmwZq1evZvfu3fzlL385sI9PPvmEsrIybrzxRq6++mruv/9+Vq5cmZPjXhed6YuEoJ4z8lw5+eSTufHGG5k0aRLnnnvugUHUzj///AP379q1i44dO9KxY0fatWvH9u3bD5wR5+XlcdRRR3HmmWeybNmyjOsPP/zwL+175MiRdOrUCYC+ffuyfv16iouLa60zLy+Piy66CIAXX3yR5cuXc+qppwKwe/dujjzySLp168auXbvYuXMnGzZs4Nvf/jaLFi1i8eLFfOtb3wJg4cKFTJkyhY8//pht27bRr18/zjvvPODzcf+3b9/O9u3bD/xHMXbs2Iwjh+aCQl9EcuaEE05gxYoVzJs3j5tvvpmRI0cC0K5dOwDatGlzYLn6dlLXvNP7zcvLq7PfgoKCA/PrujtXXXUVd9xxx5faDRs2jEceeYQTTzyRVCrFjBkzWLJkCffccw979uzhe9/7HmVlZRQXF3Prrbe2ynH/dXlHRHJm06ZNHHbYYVxxxRVMnDiRFStWZLVdKpVi1qxZ7N+/ny1btrBo0SJOO+20jOs7duyY2DDKI0eOZPbs2QfG+t+2bRvr168/UFf1uP8lJSUsXLiQdu3a0alTpwMB37VrV3bt2sXs2bNr7b9z58507tyZV155BYAnn3wykbqzpTN9EcmZVatWMXHiRNq0aUN+fj4PPvggF198cb3bXXjhhSxZsoSBAwdiZkyZMoVu3bplXF9YWEheXh4DBw5k3LhxdOnSpdE19+3bl1/+8pecffbZfPbZZ+Tn5zNt2jSOO+44UqkUGzZsYPjw4eTl5VFcXEyfPn2AKMy/+93v0r9/f7p163bg8lBtHnnkEa655hrMrNnfyNV4+iKHKI2nf/DTePoiItIkurwjIsEYMmQIe/fu/cK6xx9/nJNPPrmFKmp+Cn2RQ5i7Y2YtXUar8dprr7V0CVnL1aV3Xd4ROUQVFBSwdevWnIWH5I67s3XrVgoKChLvW2f6Ioeo7t27U1VVxZYtW1q6FGmEgoICunfvnni/Cn2RQ1R+fj69evVq6TKkldHlHRGRgCj0RUQCotAXEQlIvaFvZsVmttDM3jKzNWb2g1ramJlNNbMKM3vTzAbnplwREWmKbN7I3Qfc6O4rzKwjsNzMnnf3t9LanAP0jn+GAA/G/4qISCtS75m+u2929xXx8k5gLXBsjWYXAI95ZCnQ2cyOTrxaERFpkgZd0zeznkAJUPNrbccC6dPSVPHl/xhERKSFZR36ZtYB+BPwQ3ff0Zidmdl4MyszszJ9YUREpPllFfpmlk8U+E+6+5xammwE0uch6x6v+wJ3n+7upe5eWlRU1Jh6RUSkCbL59I4BvwPWuvu9GZrNBa6MP8VzOvChu29OsE4REUlANp/eOQMYC6wys/J43b8BPQDc/SFgHjAaqAA+Bq5OvFIREWmyekPf3V8B6hyb1aNh/P41qaJERCQ39I1cEZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIPWGvpnNMLMPzGx1hvtHmNmHZlYe//ws+TJFRCQJbbNoMxN4AHisjjaL3f3cRCoSEZGcqfdM390XAduaoRYREcmxpK7pDzWzlWY238z6JdSniIgkLJvLO/VZARzn7rvMbDTwZ6B3bQ3NbDwwHqBHjx4J7FpERBqiyWf67r7D3XfFy/OAfDPrmqHtdHcvdffSoqKipu5aREQaqMmhb2bdzMzi5dPiPrc2tV8REUlevZd3zOwpYATQ1cyqgFuAfAB3fwi4GLjezPYBu4HL3N1zVrGIiDRavaHv7mPquf8Boo90iohIK6dv5IqIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBKTe0DezGWb2gZmtznC/mdlUM6swszfNbHDyZYqISBKyOdOfCYyq4/5zgN7xz3jgwaaXJSIiuVBv6Lv7ImBbHU0uAB7zyFKgs5kdnVSBIiKSnCSu6R8LbEi7XRWv+xIzG29mZWZWtmXLlgR2LSIiDdGsb+S6+3R3L3X30qKioubctYiIkEzobwSK0253j9eJiEgrk0TozwWujD/FczrwobtvTqBfERFJWNv6GpjZU8AIoKuZVQG3APkA7v4QMA8YDVQAHwNX56pYERFpmnpD393H1HO/A/+aWEUiIpIz+kauiEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISkKxC38xGmdk7ZlZhZjfVcv84M9tiZuXxz/9KvlQREWmqtvU1MLM8YBrwNaAKWGZmc939rRpNZ7n7hBzUKCIiCcnmTP80oMLd33P3T4A/ABfktiwREcmFbEL/WGBD2u2qeF1NF5nZm2Y228yKa+vIzMabWZmZlW3ZsqUR5YqISFMk9UbuM0BPdx8APA88Wlsjd5/u7qXuXlpUVJTQrkVEJFvZhP5GIP3MvXu87gB33+rue+ObvwVOSaY8ERFJUjahvwzobWa9zOwrwGXA3PQGZnZ02s3zgbXJlSgiIkmp99M77r7PzCYAzwF5wAx3X2NmtwFl7j4X+L6ZnQ/sA7YB43JYs4iINJK5e4vsuLS01MvKylpk3yIiByszW+7upY3dXt/IFREJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEApJV6JvZKDN7x8wqzOymWu5vZ2az4vtfM7OeiVcqIiJNVm/om1keMA04B+gLjDGzvjWafQf4u7sfD/wa+FXShYqISNNlc6Z/GlDh7u+5+yfAH4ALarS5AHg0Xp4NjDQzS65MERFJQjahfyywIe12Vbyu1jbuvg/4ECis2ZGZjTezMjMr27JlS+MqFhGRRmvWN3Ldfbq7l7p7aVFRUXPuWkREyC70NwLFabe7x+tqbWNmbYFOwNYkChQRkeRkE/rLgN5m1svMvgJcBsyt0WYucFW8fDHwkrt7cmWKiEgS2tbXwN33mdkE4DkgD5jh7mvM7DagzN3nAr8DHjezCmAb0X8MIiLSytQb+gDuPg+YV2Pdz9KW9wD/kmxpIiKSNGupqzBmtgVYn3C3XYH/SbjPJLTWukC1NUZrrQtUW2O01rqg9tqOc/dGfxKmxUI/F8yszN1LW7qOmlprXaDaGqO11gWqrTFaa12Qm9o09o6ISEAU+iIiATnUQn96SxeQQWutC1RbY7TWukC1NUZrrQtyUNshdU1fRETqdqid6YuISB0U+iIiAWm1oW9mM8zsAzNbnbZukJktNbPyeLTO0+L1E+N15Wa22sz2m9kRtfQ508zeT2s7KKG6BprZEjNbZWbPmNnhafdNjieXecfMvp6hz17x5DMV8WQ0X2loXQ2tzcy+ZmbL4/XLzeysDH3eamYb047Z6GaoraeZ7U7b50MZ+jzCzJ43s3fjf7vkuK7L02oqN7PPansOJXjMis1soZm9ZWZrzOwHdT1ui0yNn0dvmtngDP2eEj+2irh9g4ZBb0Rdl8f1rDKz/zKzgRn6TeL12dDaRpjZh2n7/FmGfpv0Gm1EXbnLNHdvlT/AcGAwsDpt3QLgnHh5NPByLdudRzT2T219zgQuzkFdy4Az4+VrgF/Ey32BlUA7oBewDsirpc+ngcvi5YeA65uhthLgmHi5P7AxQ5+3Aj/O0e8zU20909vV0ecU4KZ4+SbgV7msq8Z2JwPrcnzMjgYGx8sdgb/Gz6laH3f8mpgPGHA68FqGfl+P77e4/Tk5rmsY0CVePqeOupJ4fTa0thHAX7Lot0mv0YbWVWPbRDOt1Z7pu/sionF8vrAaqD6L7gRsqmXTMcBTzVzXCcCiePl54KJ4+QLgD+6+193fByqIJqU5ID7LOoto8hmIJqP5Zq5rc/c33L36+K0Bvmpm7Rqz36Rra4D0yXsaddyaUNcYogmFcsbdN7v7inh5J7CWaO6KTI/7AuAxjywFOpvZ0el9xrcPd/elHqXGYzTwuDW0Lnf/L3f/e7x+KdFIvTnRiGNWryReo02sK9FMa7Whn8EPgbvMbANwNzA5/U4zOwwYBfypjj5uj//U/HWCIbeGz2cT+xc+H4o6mwloCoHtHk0+k6lNLmpLdxGwwt33ZuhjQnzMZjTmEkoja+tlZm+Y2f81s1SG7Y9y983x8n8DRzVDXdUupe4XYqLHzKJ5p0uA18j8uLOd8KiqnjZJ15XuO0R/XWSS2OuzAbUNNbOVZjbfzPrV0lWir9GGHLNcZNrBFvrXAze4ezFwA9HonunOA15195pnbtUmA32AU4EjgEkJ1XUN8D0zW070p9snCfWbhDpri5/kvwKuzbD9g8A/AoOAzcA9zVDbZqCHu5cAPwJ+b2nvk9QmPmtN6vPH9R2zIcDH7r66to1J+JiZWQeiF/0P3X1H+n0JP+6c1mVm/0wU+pled4m9PhtQ2wqisWwGAvcDf27sPhOuq1rymdaU62e5/qHGtV2iaRirv1tgwI4a7f838O0s+x5BFtfysqmrxn0nAK/Hy5OByWn3PQcMrdHeiAZUahvfHgo8l9Qxy1RbfLs70bXFM5rad9K11bjvZaC0lvXvAEfHy0cD7zRHXcCvgX9rpmOWHz9vflTf4wYeBsbU1i5t3dHA22m3xwAP57Ku+PYAove0Tsiy/6a8PhtUW41tK4GuNdYl8hptTF3kINMOtjP9TcCZ8fJZwLvVd5hZp/i+/5Np4+rrm/E1um8Cmc7UGsTMjoz/bQPcTPRGD0STy1xmZu3MrBfQm+hNtAM8+m0tJJp8BqLJaDI+hqRqM7POwH8SvYn0ah3bp18TvpCEjlk9tRWZWV68/A9Ex+29WrpIn7wnseNWx++zet0l1HE9P6ljFj9Pfwesdfd70+7K9LjnAlda5HTgQ//80gEQXVsGdpjZ6XH/V9LA49bQusysBzAHGOvuf62j3ya/PhtRW7d4Gyz6NGAbasz6l8RrtBG/y9xlWmPPQHL9Q3S9dDPwKdE1tO8A/wQsJ/pEzGvAKWntxxG9aVqzn3l8/imVl4BV8YF5AuiQUF0/IDpj/itwJ/FfI3H7fyc6w3mHtE9J1KjrH4j+M6gA/gi0S/CY1VobUZh9BJSn/RwZ3/db4jNr4PH4mL1J9AQ9uhlqu4jouno50Z/f56X1k15bIfAi0X/+LwBHNMPvcwSwtJZ+cnHM/onoz/03035HozM9bqIz0mnx820VaX8dAeVpy6VEr4F1wAPpjy9Hdf0W+Hta27Icvj4bWtuE+Lm2kuhN5mG5eI02tK54m3HkINM0DIOISEAOtss7IiLSBAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFADPrEw9N+4aZ/WNL1yOSKwp9kcg3gdnuXuLu66pXxt9u1etEDhl6MkurZdFkKmvN7D8smnhigZl91cxeNrPSuE1XM6uMl8eZ2Z8tmoyi0swmmNmP4rP3pVbLJBTxdqOJRnC93qKJLnpaNOnNY0TfdCw2swctmrhnjZn9PG3bSjO7wz6f2GewmT1nZuvM7Lq0dhPNbFk8GuLP43Xtzew/4xEeV5vZpbk6liLVFPrS2vUGprl7P2A79Y+53x/4FtGog7cTjYZZAiwhGmfmS9x9HtH4Or92939O2+9v3L2fu68H/t3dS4kGDjvTzAakdfH/3H0QsJh4UguiSUqqw/3suL/TiEbePMXMhhMNmbvJ3Qe6e3/g2WwOiEhTKPSltXvf3cvj5eVEo1bWZaG773T3LUSjsj4Tr1+Vxbbp1ns0EUm1S8xsBfAG0I9o1qNqc9P28Vra/vfGA9udHf+8QTSWUB+i/wRWAV8zs1+ZWcrdP2xAfSKN0ralCxCpR/rELvuBrwL7+PyEpaCO9p+l3f6Mhj3fP6peiEdI/TFwqrv/3cxm1thv+j5q7r8t0UBod7j7wzV3YtE8tqOBX5rZi+5+WwNqFGkwnenLwagSOCVevriOdkk5nOg/gQ/N7CiieV4b4jngmngCDczsWDM70syOIbr89ARwF9FcvSI5pTN9ORjdDTxtZuOJ5gTIKXdfaWZvAG8TTUeYcf6BDNsvMLOTgCXx0O27gCuA44mm//yMaGjn6xMtXKQWGlpZRCQgurwjIhIQXd6RoJjZNOCMGqvvc/dHWqIekeamyzsiIgHR5R0RkYAo9EVEAqLQFxEJiEJfRCQg/x9IUWXz5Tf17gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "informed-costa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 146/200000 [31:56<689:25:51, 12.42s/it, episode=145, num_frames=28762, smooth_reward=-5.42e+3, reward=-2.63e+3, policy_loss=240]    /home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in AddmmBackward. Traceback of forward call that caused the error:\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2894, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3165, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-714-de02e68c717c>\", line 3, in <module>\n",
      "    df = run_experiment(acmodel_args, score_threshold=0)\n",
      "  File \"<ipython-input-710-24b41a624c9f>\", line 29, in run_experiment\n",
      "    policy_loss, value_loss = ppo.update(rollouts)\n",
      "  File \"<ipython-input-703-6e3c6115d1dd>\", line 28, in update\n",
      "    pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
      "  File \"<ipython-input-703-6e3c6115d1dd>\", line 45, in _compute_policy_loss_ppo\n",
      "    dist, _, _ = self.acmodel(obs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-701-f7d76c5da725>\", line 122, in forward\n",
      "    forward_actor = self.actor(obs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 119, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 94, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/kying/6.884/finrl/venv/lib/python3.8/site-packages/torch/nn/functional.py\", line 1753, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      " (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "  0%|          | 146/200000 [32:03<731:23:46, 13.17s/it, episode=145, num_frames=28762, smooth_reward=-5.42e+3, reward=-2.63e+3, policy_loss=240]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'AddmmBackward' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-714-de02e68c717c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# recurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0macmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_tickers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_horizon'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_ta_indicators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recurrent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-710-24b41a624c9f>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(acmodel_args, max_episodes, score_threshold)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-703-6e3c6115d1dd>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lol todo are we supposed to retain graph?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/6.884/finrl/venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/6.884/finrl/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'AddmmBackward' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "# recurrent\n",
    "acmodel_args = {'num_tickers': 4, 'time_horizon': 5, 'num_ta_indicators': 0, 'recurrent': True, 'hidden_size': 16}\n",
    "df = run_experiment(acmodel_args, score_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-metro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl2",
   "language": "python",
   "name": "finrl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
