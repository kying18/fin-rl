{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "modified-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "seeing-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_inputs, action_dim, recurrent, batch_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            self.hidden = self.init_hidden(batch_size, hidden_size)\n",
    "            self.gru = nn.GRU(num_inputs, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "        else:\n",
    "            self.fwd = nn.Sequential(nn.Linear(num_inputs, hidden_size), nn.Tanh())\n",
    "        \n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.dist = DiagonalGaussian(action_dim)\n",
    "\n",
    "        self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h=None):\n",
    "        if self.recurrent:\n",
    "            if rnn_h is None:\n",
    "                rnn_h = self.hidden\n",
    "            # TODO figure out dimensionality\n",
    "            obs, rnn_h = self.gru(obs.unsqueeze(0), rnn_h)\n",
    "        else:\n",
    "            obs = self.fwd(obs)\n",
    "            \n",
    "        action_dist = self.dist.evaluate(self.actor(obs))\n",
    "        \n",
    "        forward_critic = self.critic(obs)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "demographic-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.FloatTensor([[1, 2, 3, 4]])\n",
    "acmodel = ACModel(4, 2, False, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "noble-insert",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist, value, _ = acmodel(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "brave-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1347, -1.6704]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fuzzy-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.995, gae_lambda=0.95, device=None):\n",
    "        self.episode_length = env.episode_length\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.values = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.log_probs = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.obss = [None] * self.episode_length\n",
    "        \n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        return obs\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        obs = env.reset()\n",
    "        total_return = 0\n",
    "        T = 0\n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.acmodel(obs)\n",
    "                \n",
    "            action = dist.sample()\n",
    "            \n",
    "            self.obss[T] = obs\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            total_return += reward\n",
    "            \n",
    "            self.actions[T] = action\n",
    "            self.values[T] = value\n",
    "            self.rewards[T] = reward\n",
    "            # TODO figure out log probs..\n",
    "            # self.log_probs[T] = dist.log_prob(action)\n",
    "            \n",
    "            \n",
    "            T += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        self.actions = self.actions[:T]\n",
    "        self.values = self.values[:T]\n",
    "        self.rewards = self.rewards[:T]\n",
    "        self.log_probs = self.log_probs[:T]\n",
    "        self.gaes = self.compute_advantage_gae(T)\n",
    "            \n",
    "    def compute_advantage_gae(self, T):\n",
    "        def _delta(t, rewards, discount, values):\n",
    "            return rewards[t] + ((discount * values[t+1] - values[t]) if t+1 < values.shape[0] else 0)\n",
    "\n",
    "        advantages = torch.zeros_like(self.values)\n",
    "\n",
    "        n = self.values.shape[0]\n",
    "        for t in range(n):\n",
    "            advantages[t] = sum([(self.gae_lambda*self.discount)**i * _delta(t+i, self.rewards, self.discount, self.values) for i in range(n-t)])\n",
    "\n",
    "        return advantages[:T]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=1e-3,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=5):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "        dist, _ = self.acmodel(rollouts.obss) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions)\n",
    "        \n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "        value_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "        \n",
    "        for i in range(self.train_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "            v_loss = self._compute_value_loss(rollouts.obss, rollouts.returns)\n",
    "            \n",
    "            loss = v_loss + pi_loss\n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "                \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "        \n",
    "    def _compute_policy_loss_ppo(obs, old_logp, actions, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _ = self.acmodel(obs)\n",
    "        new_logp = dist.log_prob(actions)\n",
    "\n",
    "        new_p, old_p = torch.exp(new_logp), torch.exp(old_logp)\n",
    "        r = new_p / old_p\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-args.clip_ratio, 1+args.clip_ratio)*advantages\n",
    "        min_advs = torch.minimum(r*advantages, torch.clamp(r, 1-args.clip_ratio, 1+args.clip_ratio)*advantages)\n",
    "\n",
    "        policy_loss = -torch.mean(min_advs)\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(obs, returns):\n",
    "        _, values = acmodel(obs)\n",
    "        value_loss = torch.mean((returns - values)**2)\n",
    "\n",
    "        return value_loss\n",
    "        \n",
    "    def update_parameters_ppo(optimizer, acmodel, sb, args):\n",
    "\n",
    "    \n",
    "    def _compute_value_loss(obs, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "        ##################################################################\n",
    "        _, values = acmodel(obs)\n",
    "        value_loss = torch.mean((returns - values)**2)\n",
    "\n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "temporal-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config params from the pset\n",
    "\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=False,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=False):\n",
    "        \n",
    "        self.score_threshold = score_threshold\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.log_interval = log_interval\n",
    "        self.max_episodes = max_episodes\n",
    "        self.use_critic = use_critic\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.target_kl = target_kl\n",
    "        self.train_ac_iters = train_ac_iters\n",
    "        self.gae_lambda=gae_lambda\n",
    "        self.use_discounted_reward=use_discounted_reward\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.use_gae = use_gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outside-wright",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funky-scroll",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-layout",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl2",
   "language": "python",
   "name": "finrl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
