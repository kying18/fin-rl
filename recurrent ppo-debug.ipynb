{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554800b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458fd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "236bd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_weight(network):\n",
    "    params = list(network.parameters())\n",
    "    max_val = -np.inf\n",
    "    for p in params:\n",
    "        v = torch.max(torch.abs(p))\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "    return max_val\n",
    "\n",
    "\n",
    "def get_max_grad(network):\n",
    "    max_val = -np.inf\n",
    "    for param in network.parameters():\n",
    "        if param.grad is not None:\n",
    "            v = torch.max(torch.abs(param.grad))\n",
    "            if v > max_val:\n",
    "                max_val = v\n",
    "    return max_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-classroom",
   "metadata": {},
   "source": [
    "# Architecture for ACModel, Rollouts, and PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "seeing-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these following functions were coded with reference to\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
    "\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def init_gru_params(gru):\n",
    "    for name, param in gru.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.orthogonal_(param)\n",
    "\n",
    "# this is from https://github.com/p-morais/deep-rl/blob/master/rl/distributions/gaussian.py\n",
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(self, num_outputs, init_std=1, learn_std=True):\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "\n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.ones(1, num_outputs) * np.log(init_std),\n",
    "            requires_grad=learn_std\n",
    "        )\n",
    "\n",
    "        self.learn_std = learn_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x\n",
    "        \n",
    "#         print(self.logstd.sum())\n",
    "        std = self.logstd.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        if deterministic is False:\n",
    "            action = self.evaluate(x).sample()\n",
    "        else:\n",
    "            action, _ = self(x)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        mean, std = self(x)\n",
    "        output = torch.distributions.Normal(mean, std)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_tickers, time_horizon, num_ta_indicators, recurrent, hidden_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num tickers, time horizon, and num ta used to compute the number of inputs\n",
    "        # for recurrent network, the input size to GRU is just the num tickers (prices at each timestep)\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        # for feedforward, the input size is num tickers * time horizon\n",
    "        # and input size to the actor/critic is the current cash, holdings, and technical analysis for each ticker\n",
    "        self.num_tickers = num_tickers\n",
    "        self.time_horizon = time_horizon\n",
    "        self.num_ta_indicators = num_ta_indicators\n",
    "        \n",
    "        # TODO changed for cartpole\n",
    "        # action_dim = num_tickers + 1 # buy/sell for each ticker + 1 for cash\n",
    "        action_dim = 1\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            self.hidden_actor = self.init_hidden(time_horizon, hidden_size)\n",
    "            self.hidden_critic = self.init_hidden(time_horizon, hidden_size)\n",
    "            \n",
    "            num_inputs = num_tickers\n",
    "            \n",
    "            self.gru_actor = nn.GRU(num_inputs, hidden_size)\n",
    "            self.gru_critic = nn.GRU(num_inputs, hidden_size)\n",
    "            \n",
    "            init_gru_params(self.gru_actor)\n",
    "            init_gru_params(self.gru_critic)\n",
    "        else:\n",
    "            num_inputs = num_tickers * time_horizon\n",
    "            \n",
    "            self.fwd_actor = nn.Sequential(nn.Linear(num_inputs, hidden_size), nn.Tanh())\n",
    "            self.fwd_critic = nn.Sequential(nn.Linear(num_inputs, hidden_size), nn.Tanh())\n",
    "        \n",
    "        # output of gru/fwd, cash, holdings, and then all the ta indicators\n",
    "        # TODO changed for cartpole\n",
    "        # num_inputs = hidden_size + 1 + num_tickers + num_tickers * num_ta_indicators\n",
    "        num_inputs = hidden_size\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.dist = DiagonalGaussian(action_dim, learn_std=False)\n",
    "\n",
    "        self.apply(init_params)\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # h0 should be of shape (num_layers * num_directions, batch size, hidden_size)\n",
    "        # num_layers is 1, and RNN is not bidirectional, so num_dir = 1\n",
    "        # (1, batch_size, hidden size)\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        return nn.Parameter(h, requires_grad=True)\n",
    "\n",
    "    def forward(self, obs, rnn_h_a=None, rnn_h_c=None):\n",
    "        # suppose obs is just a vector of previous prices\n",
    "        price_obs = obs[:,:self.num_tickers * self.time_horizon]\n",
    "        other_obs = obs[:,self.num_tickers * self.time_horizon:]\n",
    "        \n",
    "        if self.recurrent:\n",
    "            if rnn_h_a is None:\n",
    "                rnn_h_a = self.hidden_actor\n",
    "            if rnn_h_c is None:\n",
    "                rnn_h_c = self.hidden_critic\n",
    "            \n",
    "            obs = torch.reshape(price_obs, (-1, self.time_horizon, self.num_tickers))\n",
    "            obs_actor, rnn_h_a = self.gru_actor(obs, rnn_h_a)\n",
    "\n",
    "            obs_actor = obs_actor[:,-1,:] # selecting the last element\n",
    "            \n",
    "            obs_critic, rnn_h_c = self.gru_critic(obs, rnn_h_c)\n",
    "            obs_critic = obs_critic[:,-1,:] # selecting the last element\n",
    "        else:\n",
    "            obs_actor = self.fwd_actor(price_obs)\n",
    "#             print ('maxv:', get_max_weight(self.fwd_actor))\n",
    "#             print ('obsact:', torch.flatten(obs_actor))\n",
    "            \n",
    "#             print ('obs:', price_obs.shape)\n",
    "#             print ('obs_actor:', obs_actor.shape)\n",
    "            obs_critic = self.fwd_critic(price_obs)\n",
    "            \n",
    "#         obs_actor = torch.cat((obs_actor, other_obs), 1)\n",
    "#         obs_critic = torch.cat((obs_critic, other_obs), 1)\n",
    "        \n",
    "        forward_actor = self.actor(obs_actor)\n",
    "        action_dist = self.dist.evaluate(forward_actor)\n",
    "        \n",
    "        forward_critic = self.critic(obs_critic)\n",
    "        \n",
    "        return action_dist, forward_critic, rnn_h_a, rnn_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fuzzy-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, acmodel, env, discount=0.995, gae_lambda=0.95, device=None):\n",
    "        # TODO changed for cartpole\n",
    "        # self.episode_length = env.episode_length\n",
    "        self.episode_length = 200\n",
    "        self.device = device\n",
    "        self.acmodel = acmodel\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.actions = None\n",
    "        self.values = None\n",
    "        self.rewards = None\n",
    "        self.log_probs = None\n",
    "        self.obss = None\n",
    "        self.gaes = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.values = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.rewards = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.log_probs = torch.zeros(self.episode_length, device=self.device)\n",
    "        self.obss = [None] * self.episode_length\n",
    "    \n",
    "    def process_obs(self, obs):\n",
    "        # TODO: formatting stuff\n",
    "        if isinstance(obs, list):\n",
    "            obs = np.stack(obs)\n",
    "\n",
    "        if len(obs.shape) == 1: # 1 dimensional\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            \n",
    "        return torch.FloatTensor(obs)\n",
    "    \n",
    "    def collect_experience(self):\n",
    "        obs = env.reset()\n",
    "        total_return = 0\n",
    "        T = 0\n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                dist, value, _, _ = self.acmodel(self.process_obs(obs))\n",
    "                \n",
    "            action = dist.sample()\n",
    "            \n",
    "            self.obss[T] = obs\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_return += reward\n",
    "            \n",
    "            self.actions[T] = action\n",
    "            self.values[T] = value\n",
    "            self.rewards[T] = float(reward)\n",
    "            self.log_probs[T] = dist.log_prob(action)\n",
    "            \n",
    "            \n",
    "            T += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        self.actions = self.actions[:T]\n",
    "        self.values = self.values[:T]\n",
    "        self.rewards = self.rewards[:T]\n",
    "        self.log_probs = self.log_probs[:T]\n",
    "        self.obss = self.process_obs(self.obss[:T])\n",
    "        self.gaes = self.compute_advantage_gae(T)\n",
    "        \n",
    "\n",
    "        \n",
    "        return total_return, T\n",
    "            \n",
    "    def compute_advantage_gae(self, T):\n",
    "        def _delta(t, rewards, discount, values):\n",
    "            return rewards[t] + ((discount * values[t+1] - values[t]) if t+1 < values.shape[0] else 0)\n",
    "\n",
    "        advantages = torch.zeros_like(self.values)\n",
    "\n",
    "        n = self.values.shape[0]\n",
    "        for t in range(n):\n",
    "            advantages[t] = sum([(self.gae_lambda*self.discount)**i * _delta(t+i, self.rewards, self.discount, self.values) for i in range(n-t)])\n",
    "\n",
    "        return advantages[:T]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neural-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rollout = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "satisfactory-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,\n",
    "                 acmodel,\n",
    "                 clip_ratio=0.2,\n",
    "                 entropy_coef=0.01,\n",
    "                 lr=1e-3,\n",
    "                 target_kl=0.01,\n",
    "                 train_iters=5):\n",
    "        \n",
    "        self.acmodel = acmodel\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl=target_kl\n",
    "        self.train_iters = train_iters\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(acmodel.parameters(), lr=lr)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        # rollouts should be RolloutBuffer object\n",
    "        dist, _, _, _ = self.acmodel(rollouts.obss) # TODO may need to process these observations\n",
    "        old_logp = dist.log_prob(rollouts.actions.view(-1,self.acmodel.action_dim)).detach()\n",
    "        \n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "        value_loss = self._compute_value_loss(rollouts.obss, rollouts.rewards)\n",
    "        \n",
    "        for i in range(self.train_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "#             try:\n",
    "            pi_loss, approx_kl = self._compute_policy_loss_ppo(rollouts.obss, old_logp, rollouts.actions, rollouts.gaes)\n",
    "#             except:\n",
    "#                 print(i)\n",
    "#                 print(\"rollout obs\", rollouts.obss)\n",
    "#                 global bad_rollout\n",
    "#                 global acmodel\n",
    "#                 bad_rollout = rollouts\n",
    "#                 acmodel = self.acmodel\n",
    "                \n",
    "            v_loss = self._compute_value_loss(rollouts.obss, rollouts.rewards)\n",
    "            #print ('vloss:', v_loss, 'pi_loss:', pi_loss)\n",
    "            loss = v_loss + pi_loss\n",
    "            \n",
    "            if approx_kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "            \n",
    "            loss.backward(retain_graph=True) # lol todo are we supposed to retain graph?\n",
    "\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "        \n",
    "    def _compute_policy_loss_ppo(self, obs, old_logp, actions, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "        \n",
    "\n",
    "        dist, _, _, _ = self.acmodel(obs)\n",
    "        \n",
    "        new_logp = dist.log_prob(actions.view(-1,self.acmodel.action_dim))\n",
    "\n",
    "#         print(new_logp[0])\n",
    "\n",
    "#         new_p, old_p = torch.exp(new_logp), torch.exp(old_logp.detach())\n",
    "#         r = new_p / old_p\n",
    "\n",
    "        \n",
    "\n",
    "        r = torch.exp(new_logp - old_logp.detach())\n",
    "\n",
    "        clamp_adv = torch.clamp(r, 1-self.clip_ratio, 1+self.clip_ratio)*advantages.view(-1,1)\n",
    "        \n",
    "        min_advs = torch.minimum(r*advantages.view(-1,1), clamp_adv)\n",
    "\n",
    "        policy_loss = -torch.mean(min_advs)\n",
    "        approx_kl = (old_logp - new_logp).mean()\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, values, _, _ = self.acmodel(obs)\n",
    "             \n",
    "\n",
    "        value_loss = torch.mean((returns.view(-1,1) - values.view(-1,1))**2)\n",
    "        \n",
    "        return value_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d22fac5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACModel(\n",
       "  (fwd_actor): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (fwd_critic): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (actor): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (dist): DiagonalGaussian()\n",
       ")"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "compact-engagement",
   "metadata": {},
   "source": [
    "# Cartpole Gym Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "outside-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import registry, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "adult-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "        \n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "    \n",
    "    def _qdotdot(self, q, u):\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "        \n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "        \n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) - \n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "                \n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "    \n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "    \n",
    "    def step(self, q, u):\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)    \n",
    "            \n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "            \n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "            \n",
    "        return new_q\n",
    "    \n",
    "    # given q [n, q_shape] and u [n, t] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        qs = [q]\n",
    "        \n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "                \n",
    "        return torch.stack(qs, dim=1)\n",
    "    \n",
    "    def reward(self, q, u):\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "                        \n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "    \n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, time_horizon=5, timestep_limit=200):\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "        \n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.time_horizon = time_horizon\n",
    "#         self.memory = np.zeros(4 * time_horizon)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "        \n",
    "        for t in range(self.time_horizon-1):\n",
    "            # start with a few random actions to initialize\n",
    "            action = np.random.uniform(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "            self.step(action)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.q_sim\n",
    "            \n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "        \n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "        \n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "        \n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "        \n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "        \n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "        \n",
    "        return np.array(self.traj[-self.time_horizon:]).flatten(), reward, done, {}\n",
    "    \n",
    "    def is_done(self):\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "    \n",
    "env_name = 'CartpoleWithMemory-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n",
    "env = gym.make('CartpoleWithMemory-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incomplete-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.00731243,\n",
       "       -0.01218738,  0.36562141, -0.60936902,  0.00287034, -0.00470564,\n",
       "       -0.22210465,  0.37408685,  0.00852489, -0.01402069,  0.28272764,\n",
       "       -0.46575225,  0.01237083, -0.02022999,  0.1922972 , -0.31046505])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excessive-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60.555091857910156, 31.47611427307129)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recurrent model\n",
    "acmodel = ACModel(num_tickers=4, time_horizon=5, num_ta_indicators=0, recurrent=True, hidden_size=32)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chief-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1832.9512939453125, 23879.939453125)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feedforward model\n",
    "acmodel = ACModel(num_tickers=4, time_horizon=5, num_ta_indicators=0, recurrent=False, hidden_size=32)\n",
    "rollouts = RolloutBuffer(acmodel, env)\n",
    "rollouts.collect_experience()\n",
    "ppo = PPO(acmodel)\n",
    "ppo.update(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-bottle",
   "metadata": {},
   "source": [
    "Ok so looks like the Cartpole gym syntactically works.. Let's try to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-optimum",
   "metadata": {},
   "source": [
    "# Run experiment on Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "selected-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 6.884 HW4\n",
    "\n",
    "def run_experiment(acmodel, acmodel_kwargs, ppo_kwargs, max_episodes=200000, score_threshold=0.8):\n",
    "    # acmodel_args should be dictionary corresponding to inputs of acmodel\n",
    "    # ie {num_tickers: 4, time_horizon: 5, etc..}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make('CartpoleWithMemory-v0')\n",
    "    \n",
    "    \n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "    \n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "    \n",
    "    num_frames = 0\n",
    "    rollouts = RolloutBuffer(acmodel, env)\n",
    "    ppo = PPO(acmodel, **ppo_kwargs)\n",
    "\n",
    "    pbar = tqdm(range(max_episodes))\n",
    "    for update in pbar:\n",
    "        rollouts.reset() # resetting the buffer\n",
    "        total_return, T = rollouts.collect_experience()\n",
    "        policy_loss, value_loss = ppo.update(rollouts)\n",
    "        \n",
    "        num_frames += T\n",
    "        rewards.append(total_return)\n",
    "        \n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':total_return, 'policy_loss': policy_loss}\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "    else:\n",
    "        print('Unsolved. Check your implementation.')\n",
    "    \n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8211d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080f052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "complimentary-correspondence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 184/200 [01:33<00:08,  1.96it/s, episode=184, num_frames=36445, smooth_reward=20.2, reward=26.4, policy_loss=-2.35]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# feedforward\n",
    "acmodel_kwargs = {'num_tickers': 4, 'time_horizon': 1, 'num_ta_indicators': 0, 'recurrent': False, 'hidden_size': 64}\n",
    "acmodel = ACModel(**acmodel_kwargs)\n",
    "ppo_kwargs = {'lr': 1e-2}\n",
    "df = run_experiment(acmodel, acmodel_kwargs, ppo_kwargs, max_episodes=200, score_threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7e95c423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x138dd5be0>]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df.index, df['smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "senior-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fdcc7009700>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "robust-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "acmodel_kwargs = {'num_tickers': 4, 'time_horizon': 1, 'num_ta_indicators': 0, 'recurrent': False, 'hidden_size': 64}\n",
    "ppo_kwargs = {'lr': 1e-3}\n",
    "acm = ACModel(**acmodel_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd342882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9501, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(list(acm.parameters())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "vital-approach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<SelectBackward>)\n",
      "tensor(nan, grad_fn=<SelectBackward>) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "dist, val, _, _ = acmodel(bad_rollout.obss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-marks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "informed-palestine",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd_actor.0.weight Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "fwd_actor.0.bias Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       requires_grad=True)\n",
      "fwd_critic.0.weight Parameter containing:\n",
      "tensor([[-0.1554, -0.1525, -0.0149,  ...,  0.2803,  0.1742, -0.2377],\n",
      "        [-0.2915, -0.2077,  0.3201,  ..., -0.0714, -0.1693, -0.2199],\n",
      "        [ 0.1024, -0.0691,  0.0253,  ...,  0.0669,  0.0149, -0.2636],\n",
      "        ...,\n",
      "        [-0.2078, -0.0824,  0.0546,  ..., -0.1069, -0.0641, -0.2592],\n",
      "        [ 0.0145, -0.0209,  0.3356,  ...,  0.0996, -0.2404,  0.4291],\n",
      "        [-0.1941, -0.1300, -0.1999,  ...,  0.3586,  0.0224, -0.0121]],\n",
      "       requires_grad=True)\n",
      "fwd_critic.0.bias Parameter containing:\n",
      "tensor([-0.0110,  0.0102, -0.0138, -0.0112, -0.0112, -0.0101, -0.0098, -0.0036,\n",
      "        -0.0098,  0.0150, -0.0107,  0.0094, -0.0130,  0.0024, -0.0088,  0.0115,\n",
      "        -0.0093, -0.0130, -0.0109,  0.0107, -0.0101,  0.0105,  0.0040,  0.0174,\n",
      "        -0.0118, -0.0127,  0.0109,  0.0146, -0.0121,  0.0104,  0.0105, -0.0104,\n",
      "         0.0089,  0.0114, -0.0110,  0.0126,  0.0093, -0.0093, -0.0102,  0.0121,\n",
      "        -0.0147,  0.0106, -0.0117,  0.0114,  0.0108,  0.0112,  0.0126,  0.0105,\n",
      "         0.0103,  0.0103, -0.0136,  0.0066,  0.0093,  0.0138,  0.0151, -0.0067,\n",
      "        -0.0114,  0.0104, -0.0098,  0.0096, -0.0111, -0.0119,  0.0080,  0.0108],\n",
      "       requires_grad=True)\n",
      "actor.0.weight Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "actor.0.bias Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       requires_grad=True)\n",
      "actor.2.weight Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "actor.2.bias Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       requires_grad=True)\n",
      "actor.4.weight Parameter containing:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       requires_grad=True)\n",
      "actor.4.bias Parameter containing:\n",
      "tensor([nan], requires_grad=True)\n",
      "critic.0.weight Parameter containing:\n",
      "tensor([[ 0.1322, -0.1638, -0.2577,  ...,  0.0281,  0.0740, -0.1032],\n",
      "        [ 0.0289,  0.0992,  0.1480,  ...,  0.0085,  0.0162, -0.2812],\n",
      "        [-0.0050,  0.0107,  0.0803,  ...,  0.2817, -0.0382,  0.0435],\n",
      "        ...,\n",
      "        [-0.0024,  0.1413, -0.0404,  ...,  0.1850, -0.1002,  0.0553],\n",
      "        [ 0.0848,  0.2427, -0.2510,  ..., -0.1243,  0.0140, -0.2373],\n",
      "        [ 0.1311, -0.0905,  0.0249,  ..., -0.0426, -0.1441, -0.0486]],\n",
      "       requires_grad=True)\n",
      "critic.0.bias Parameter containing:\n",
      "tensor([-0.0080,  0.0103,  0.0103, -0.0102,  0.0104,  0.0113, -0.0094,  0.0092,\n",
      "         0.0109, -0.0101, -0.0093, -0.0099, -0.0100,  0.0101,  0.0100, -0.0100,\n",
      "        -0.0097, -0.0102,  0.0101, -0.0103, -0.0068, -0.0099, -0.0101, -0.0105,\n",
      "        -0.0080, -0.0100, -0.0100, -0.0096,  0.0124, -0.0120,  0.0100, -0.0101,\n",
      "         0.0103,  0.0029,  0.0103, -0.0092,  0.0128, -0.0109,  0.0095,  0.0103,\n",
      "        -0.0114,  0.0138, -0.0108, -0.0102,  0.0096,  0.0099, -0.0103, -0.0099,\n",
      "        -0.0102,  0.0137,  0.0063,  0.0097,  0.0042, -0.0087,  0.0109,  0.0092,\n",
      "        -0.0109,  0.0116,  0.0097,  0.0098, -0.0076,  0.0105,  0.0087, -0.0094],\n",
      "       requires_grad=True)\n",
      "critic.2.weight Parameter containing:\n",
      "tensor([[-0.0003,  0.1748,  0.2297,  ..., -0.0485, -0.0254, -0.0901],\n",
      "        [-0.0336,  0.1121, -0.1059,  ..., -0.1447, -0.0048, -0.0468],\n",
      "        [ 0.0023,  0.0965,  0.0014,  ..., -0.0323,  0.0082, -0.0639],\n",
      "        ...,\n",
      "        [-0.0222,  0.0319, -0.0074,  ...,  0.1123,  0.0630, -0.0486],\n",
      "        [ 0.0131, -0.0272, -0.0732,  ...,  0.1271,  0.0237,  0.1328],\n",
      "        [ 0.0646,  0.1976,  0.0356,  ...,  0.1447,  0.0900, -0.1440]],\n",
      "       requires_grad=True)\n",
      "critic.2.bias Parameter containing:\n",
      "tensor([-0.0097,  0.0102,  0.0099, -0.0101, -0.0100, -0.0094,  0.0098, -0.0097,\n",
      "         0.0098, -0.0101,  0.0099, -0.0098,  0.0096,  0.0081, -0.0097,  0.0097,\n",
      "         0.0098,  0.0098, -0.0098, -0.0098, -0.0097,  0.0101,  0.0097,  0.0100,\n",
      "        -0.0098,  0.0094, -0.0099,  0.0097,  0.0098,  0.0100, -0.0096, -0.0097,\n",
      "         0.0098, -0.0101, -0.0098, -0.0099, -0.0098,  0.0097, -0.0099,  0.0101,\n",
      "         0.0098, -0.0096,  0.0098,  0.0098,  0.0097,  0.0099,  0.0096,  0.0098,\n",
      "         0.0098, -0.0105, -0.0097,  0.0102,  0.0107, -0.0098,  0.0097,  0.0101,\n",
      "         0.0102, -0.0101, -0.0098,  0.0099,  0.0100, -0.0097,  0.0102, -0.0097],\n",
      "       requires_grad=True)\n",
      "critic.4.weight Parameter containing:\n",
      "tensor([[ 0.0942, -0.0619, -0.0958,  0.1688,  0.1936,  0.0072, -0.0568,  0.0255,\n",
      "         -0.1139,  0.0613, -0.0978,  0.3367, -0.0259,  0.0032,  0.1473, -0.0916,\n",
      "         -0.0698, -0.0952,  0.0162,  0.1691,  0.0583, -0.0637, -0.2027, -0.2077,\n",
      "          0.1262, -0.0074,  0.1131, -0.0979, -0.3115, -0.2274,  0.1315,  0.0822,\n",
      "         -0.1496,  0.0022,  0.1093,  0.0255,  0.2516, -0.1186,  0.3176, -0.0952,\n",
      "         -0.0679,  0.0253, -0.0834, -0.0962, -0.0110, -0.0719, -0.0244, -0.0345,\n",
      "         -0.2146,  0.0185,  0.0291, -0.0864, -0.0160,  0.0918, -0.0621, -0.0501,\n",
      "         -0.1731,  0.0800,  0.0602, -0.0681, -0.0616,  0.0400, -0.1015,  0.0333]],\n",
      "       requires_grad=True)\n",
      "critic.4.bias Parameter containing:\n",
      "tensor([-0.0098], requires_grad=True)\n",
      "dist.logstd Parameter containing:\n",
      "tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in acmodel.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-belgium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-organization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "final-plymouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='num_frames'>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/200000 [04:13<791:24:35, 14.25s/it, episode=15, num_frames=3152, smooth_reward=-76.9, reward=9.94, policy_loss=-.95]   "
     ]
    }
   ],
   "source": [
    "# recurrent\n",
    "acmodel_args = {'num_tickers': 4, 'time_horizon': 5, 'num_ta_indicators': 0, 'recurrent': True, 'hidden_size': 16}\n",
    "df = run_experiment(acmodel_args, score_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-boundary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
