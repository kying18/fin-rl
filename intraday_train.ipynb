{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f158dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9206794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "NUM_PAST_STATES = 4\n",
    "EPISODE_LENGTH = 38 - NUM_PAST_STATES\n",
    "\n",
    "\n",
    "class TradeEnv(gym.Env):\n",
    "    def __init__(self, tickers):\n",
    "        super(TradeEnv, self).__init__()\n",
    "\n",
    "        self.tickers = tickers\n",
    "        self.df_list = pickle.load(open('df_list', 'rb'))\n",
    "        self.total = pd.concat(self.df_list)\n",
    "        \n",
    "        self.means = np.mean(self.total[tickers].values, axis = 0)\n",
    "        self.stds = np.std(self.total[tickers].values, axis = 0)\n",
    "\n",
    "        self.episode_length = EPISODE_LENGTH #number of trading minutes in episode\n",
    "\n",
    "        self.num_past_states = NUM_PAST_STATES #number of past days that are used in state\n",
    "\n",
    "        self.action_space = spaces.Box(low=-10, high=10, shape=(len(self.tickers) + 1,))\n",
    "                                            \n",
    "\n",
    "        obs_length = len(self.tickers)*self.num_past_states #observation due to past stacked states\n",
    "        obs_length += 1 #balance\n",
    "        obs_length += len(self.tickers) #holdings\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(obs_length,))\n",
    "        \n",
    "        \n",
    "  \n",
    "    def step(self, action_):\n",
    "        action = softmax(action_)\n",
    "        \n",
    "        \n",
    "        self.balance += np.sum(self.holdings)\n",
    "\n",
    "        self.holdings = self.balance*action[:-1]\n",
    "        self.balance = self.balance*action[-1]\n",
    "        \n",
    "        self.last_net_worth = self.balance + np.sum(self.holdings)\n",
    "\n",
    "        self.index += 1\n",
    "\n",
    "        stock_obs = self.get_stock_obs(self.index)\n",
    "        self.next_prices = stock_obs[-1]\n",
    "        perc_change = np.divide(self.next_prices, self.curr_prices)\n",
    "        self.holdings = np.multiply(self.holdings, perc_change)\n",
    "        \n",
    "        self.curr_prices = self.next_prices\n",
    "        \n",
    "\n",
    "        self.net_worth = self.balance + np.sum(self.holdings)\n",
    "\n",
    "\n",
    "        rew = self.net_worth - self.last_net_worth # reward is the delta between last net worth and current net worth\n",
    "\n",
    "        done = (self.net_worth <= 0) or (self.steps > self.episode_length)\n",
    "        self.steps += 1\n",
    "\n",
    "        \n",
    "        obs = self.get_obs(stock_obs, self.balance, self.holdings)\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        obs = self.get_obs(self.normalize_stock_obs(stock_obs), self.balance/1000.0, self.holdings/1000.0)\n",
    "\n",
    "        return obs, 200.0*rew, done, {}\n",
    "    \n",
    "    def normalize_stock_obs(self, stock_obs):\n",
    "        return np.divide(stock_obs - self.means, self.stds)\n",
    "    \n",
    "    def get_stock_obs(self, index):\n",
    "\n",
    "        \n",
    "        return self.data[index - self.num_past_states:index][self.tickers].values #stack data\n",
    "\n",
    "    def get_obs(self, stock_obs, balance, holdings):\n",
    "        return np.concatenate([stock_obs.reshape(-1,), [balance], holdings])\n",
    "        \n",
    "    def reset(self):\n",
    "        df_idx = np.random.randint(len(self.df_list))\n",
    "        self.data = self.df_list[df_idx]\n",
    "        self.steps = 0\n",
    "        self.index = NUM_PAST_STATES\n",
    "        \n",
    "        stock_obs = self.get_stock_obs(self.index)\n",
    "        self.holdings = np.zeros(len(self.tickers)) #holdings of each stock in number of shares\n",
    "        self.balance = INITIAL_BALANCE\n",
    "        self.last_net_worth = INITIAL_BALANCE\n",
    "        \n",
    "        \n",
    "\n",
    "        self.curr_prices = stock_obs[-1]\n",
    "\n",
    "        obs = self.get_obs(self.normalize_stock_obs(stock_obs), self.balance/1000.0, self.holdings/1000.0)\n",
    "        return obs  # reward, done, info can't be included\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65570f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickers = ['LLL', 'DIS', 'WM', 'BRK.B', 'FTI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d56c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradeEnv(tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6734d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c2f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304a82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"intra_tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f10750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_env = TradeEnv(tickers=tickers)\n",
    "env = Monitor(trade_env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b95f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveRewardCallback(check_freq=10000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43390566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996fbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "# policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "#                      net_arch=[dict(pi=[96, 128, 64], vf=[96, 64, 64])])\n",
    "\n",
    "policy_kwargs = dict(net_arch=[dict(vf=[256, 128, 64], pi=[256, 256, 128])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff089ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, gamma = 1.0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24618fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc818253",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learning_rate = .004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66933ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 512.73\n",
      "Num timesteps: 13245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 549.63\n",
      "Num timesteps: 23245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 838.38\n",
      "Num timesteps: 33245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 397.65\n",
      "Num timesteps: 43245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1129.22\n",
      "Num timesteps: 53245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 234.41\n",
      "Num timesteps: 63245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 905.72\n",
      "Num timesteps: 73245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 716.56\n",
      "Num timesteps: 83245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 947.58\n",
      "Num timesteps: 93245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 977.59\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 888          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 703          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013425859 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.54        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+03     |\n",
      "|    n_updates            | 104890       |\n",
      "|    policy_gradient_loss | -0.00534     |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 2.61e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 103245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 540.84\n",
      "Num timesteps: 113245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1012.13\n",
      "Num timesteps: 123245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 863.98\n",
      "Num timesteps: 133245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 514.22\n",
      "Num timesteps: 143245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 822.60\n",
      "Num timesteps: 153245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 671.51\n",
      "Num timesteps: 163245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 896.60\n",
      "Num timesteps: 173245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1042.19\n",
      "Num timesteps: 183245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 869.07\n",
      "Num timesteps: 193245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 513.25\n",
      "Num timesteps: 203245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 819.94\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 36            |\n",
      "|    ep_rew_mean          | 779           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 713           |\n",
      "|    iterations           | 100           |\n",
      "|    time_elapsed         | 287           |\n",
      "|    total_timesteps      | 204800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052039744 |\n",
      "|    clip_fraction        | 0.00479       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -6.54         |\n",
      "|    explained_variance   | 0.987         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.13e+03      |\n",
      "|    n_updates            | 105390        |\n",
      "|    policy_gradient_loss | -0.00211      |\n",
      "|    std                  | 0.729         |\n",
      "|    value_loss           | 8.97e+03      |\n",
      "-------------------------------------------\n",
      "Num timesteps: 213245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 975.40\n",
      "Num timesteps: 223245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 778.97\n",
      "Num timesteps: 233245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 446.00\n",
      "Num timesteps: 243245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 599.47\n",
      "Num timesteps: 253245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 741.34\n",
      "Num timesteps: 263245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 560.38\n",
      "Num timesteps: 273245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 873.43\n",
      "Num timesteps: 283245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1004.87\n",
      "Num timesteps: 293245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 780.51\n",
      "Num timesteps: 303245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 519.13\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 807          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 716          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 428          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032583876 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.52        |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.83e+03     |\n",
      "|    n_updates            | 105890       |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.727        |\n",
      "|    value_loss           | 1.7e+04      |\n",
      "------------------------------------------\n",
      "Num timesteps: 313245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 969.17\n",
      "Num timesteps: 323245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 329.58\n",
      "Num timesteps: 333245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 833.55\n",
      "Num timesteps: 343245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 786.80\n",
      "Num timesteps: 353245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 672.42\n",
      "Num timesteps: 363245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 876.24\n",
      "Num timesteps: 373245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 775.74\n",
      "Num timesteps: 383245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 439.51\n",
      "Num timesteps: 393245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 745.86\n",
      "Num timesteps: 403245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 625.79\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 904         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 716         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008113718 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.4e+03     |\n",
      "|    n_updates            | 106390      |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 1e+04       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 413245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1257.35\n",
      "Num timesteps: 423245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 979.16\n",
      "Num timesteps: 433245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1144.68\n",
      "Num timesteps: 443245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 846.99\n",
      "Num timesteps: 453245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1123.13\n",
      "Num timesteps: 463245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 726.64\n",
      "Num timesteps: 473245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 748.23\n",
      "Num timesteps: 483245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 814.00\n",
      "Num timesteps: 493245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1048.14\n",
      "Num timesteps: 503245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 819.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 588          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 716          |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 715          |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081161605 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.48        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+03     |\n",
      "|    n_updates            | 106890       |\n",
      "|    policy_gradient_loss | -0.00545     |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 1.4e+04      |\n",
      "------------------------------------------\n",
      "Num timesteps: 513245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 862.40\n",
      "Num timesteps: 523245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 839.53\n",
      "Num timesteps: 533245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 724.75\n",
      "Num timesteps: 543245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 966.31\n",
      "Num timesteps: 553245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1054.46\n",
      "Num timesteps: 563245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 713.03\n",
      "Num timesteps: 573245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1049.70\n",
      "Num timesteps: 583245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 779.91\n",
      "Num timesteps: 593245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1004.77\n",
      "Num timesteps: 603245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 822.20\n",
      "Num timesteps: 613245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 811.66\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 821          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 715          |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 858          |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022540663 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.47        |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.33e+03     |\n",
      "|    n_updates            | 107390       |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 1.54e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 623245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 872.92\n",
      "Num timesteps: 633245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 978.16\n",
      "Num timesteps: 643245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 725.93\n",
      "Num timesteps: 653245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 730.55\n",
      "Num timesteps: 663245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 551.39\n",
      "Num timesteps: 673245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1035.24\n",
      "Num timesteps: 683245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 472.51\n",
      "Num timesteps: 693245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 911.69\n",
      "Num timesteps: 703245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 802.79\n",
      "Num timesteps: 713245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1191.86\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 851         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 717         |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 998         |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004915946 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.53e+03    |\n",
      "|    n_updates            | 107890      |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 8.09e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 723245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 449.68\n",
      "Num timesteps: 733245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 789.24\n",
      "Num timesteps: 743245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1012.73\n",
      "Num timesteps: 753245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 951.10\n",
      "Num timesteps: 763245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 508.74\n",
      "Num timesteps: 773245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1254.06\n",
      "Num timesteps: 783245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 464.14\n",
      "Num timesteps: 793245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1019.39\n",
      "Num timesteps: 803245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 785.48\n",
      "Num timesteps: 813245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 951.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 614         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 721         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 1135        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002208367 |\n",
      "|    clip_fraction        | 0.00483     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47e+03    |\n",
      "|    n_updates            | 108390      |\n",
      "|    policy_gradient_loss | -0.000874   |\n",
      "|    std                  | 0.717       |\n",
      "|    value_loss           | 9.32e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 823245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1118.56\n",
      "Num timesteps: 833245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 865.13\n",
      "Num timesteps: 843245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 931.37\n",
      "Num timesteps: 853245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 859.88\n",
      "Num timesteps: 863245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 854.90\n",
      "Num timesteps: 873245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 411.13\n",
      "Num timesteps: 883245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1129.64\n",
      "Num timesteps: 893245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 945.77\n",
      "Num timesteps: 903245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 872.10\n",
      "Num timesteps: 913245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 727.90\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 36            |\n",
      "|    ep_rew_mean          | 1.04e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 725           |\n",
      "|    iterations           | 450           |\n",
      "|    time_elapsed         | 1270          |\n",
      "|    total_timesteps      | 921600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040225533 |\n",
      "|    clip_fraction        | 0.0083        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -6.4          |\n",
      "|    explained_variance   | 0.989         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.47e+03      |\n",
      "|    n_updates            | 108890        |\n",
      "|    policy_gradient_loss | -0.00233      |\n",
      "|    std                  | 0.714         |\n",
      "|    value_loss           | 1.51e+04      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 923245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 884.74\n",
      "Num timesteps: 933245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1112.92\n",
      "Num timesteps: 943245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 859.77\n",
      "Num timesteps: 953245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 617.93\n",
      "Num timesteps: 963245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 799.80\n",
      "Num timesteps: 973245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1162.96\n",
      "Num timesteps: 983245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 737.42\n",
      "Num timesteps: 993245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 846.69\n",
      "Num timesteps: 1003245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 832.16\n",
      "Num timesteps: 1013245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 829.14\n",
      "Num timesteps: 1023245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 701.22\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 575          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 728          |\n",
      "|    iterations           | 500          |\n",
      "|    time_elapsed         | 1406         |\n",
      "|    total_timesteps      | 1024000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024116589 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.38        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.45e+03     |\n",
      "|    n_updates            | 109390       |\n",
      "|    policy_gradient_loss | -0.0057      |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 1.28e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1033245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 735.73\n",
      "Num timesteps: 1043245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1075.89\n",
      "Num timesteps: 1053245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 403.45\n",
      "Num timesteps: 1063245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 591.75\n",
      "Num timesteps: 1073245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 551.43\n",
      "Num timesteps: 1083245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 669.24\n",
      "Num timesteps: 1093245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 814.13\n",
      "Num timesteps: 1103245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 705.55\n",
      "Num timesteps: 1113245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1003.80\n",
      "Num timesteps: 1123245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1013.74\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 907          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 728          |\n",
      "|    iterations           | 550          |\n",
      "|    time_elapsed         | 1545         |\n",
      "|    total_timesteps      | 1126400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025959918 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.37        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.22e+03     |\n",
      "|    n_updates            | 109890       |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 2e+04        |\n",
      "------------------------------------------\n",
      "Num timesteps: 1133245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 975.44\n",
      "Num timesteps: 1143245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 766.90\n",
      "Num timesteps: 1153245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 673.68\n",
      "Num timesteps: 1163245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 963.96\n",
      "Num timesteps: 1173245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 700.58\n",
      "Num timesteps: 1183245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 850.63\n",
      "Num timesteps: 1193245\n",
      "Best mean reward: 1474.71 - Last mean reward per episode: 1551.89\n",
      "Saving new best model to intra_tmp/best_model\n",
      "Num timesteps: 1203245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 697.22\n",
      "Num timesteps: 1213245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 677.03\n",
      "Num timesteps: 1223245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 634.70\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 861          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 600          |\n",
      "|    time_elapsed         | 1682         |\n",
      "|    total_timesteps      | 1228800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053344537 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.35        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.79e+03     |\n",
      "|    n_updates            | 110390       |\n",
      "|    policy_gradient_loss | -0.004       |\n",
      "|    std                  | 0.708        |\n",
      "|    value_loss           | 5.71e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1233245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 796.55\n",
      "Num timesteps: 1243245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1201.16\n",
      "Num timesteps: 1253245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1045.03\n",
      "Num timesteps: 1263245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 392.81\n",
      "Num timesteps: 1273245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 578.79\n",
      "Num timesteps: 1283245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 888.68\n",
      "Num timesteps: 1293245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 867.76\n",
      "Num timesteps: 1303245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 424.37\n",
      "Num timesteps: 1313245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 504.51\n",
      "Num timesteps: 1323245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1195.60\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.15e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 732          |\n",
      "|    iterations           | 650          |\n",
      "|    time_elapsed         | 1817         |\n",
      "|    total_timesteps      | 1331200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031226587 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.32        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+03     |\n",
      "|    n_updates            | 110890       |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    std                  | 0.705        |\n",
      "|    value_loss           | 1.93e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1333245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1418.66\n",
      "Num timesteps: 1343245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 897.44\n",
      "Num timesteps: 1353245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 575.81\n",
      "Num timesteps: 1363245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 879.69\n",
      "Num timesteps: 1373245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 879.75\n",
      "Num timesteps: 1383245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 611.17\n",
      "Num timesteps: 1393245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 686.79\n",
      "Num timesteps: 1403245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 518.26\n",
      "Num timesteps: 1413245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 604.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1423245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 728.79\n",
      "Num timesteps: 1433245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1021.64\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.05e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 734          |\n",
      "|    iterations           | 700          |\n",
      "|    time_elapsed         | 1952         |\n",
      "|    total_timesteps      | 1433600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057401666 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.3         |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.41e+03     |\n",
      "|    n_updates            | 111390       |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    std                  | 0.702        |\n",
      "|    value_loss           | 1.2e+04      |\n",
      "------------------------------------------\n",
      "Num timesteps: 1443245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 980.46\n",
      "Num timesteps: 1453245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 995.93\n",
      "Num timesteps: 1463245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 854.56\n",
      "Num timesteps: 1473245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 628.00\n",
      "Num timesteps: 1483245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 432.22\n",
      "Num timesteps: 1493245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 677.28\n",
      "Num timesteps: 1503245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 888.06\n",
      "Num timesteps: 1513245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1172.85\n",
      "Num timesteps: 1523245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 892.04\n",
      "Num timesteps: 1533245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 627.47\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 648          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 734          |\n",
      "|    iterations           | 750          |\n",
      "|    time_elapsed         | 2091         |\n",
      "|    total_timesteps      | 1536000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | -0.000853462 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.27        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87e+03     |\n",
      "|    n_updates            | 111890       |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    std                  | 0.699        |\n",
      "|    value_loss           | 8.93e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1543245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 784.99\n",
      "Num timesteps: 1553245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 728.47\n",
      "Num timesteps: 1563245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1026.00\n",
      "Num timesteps: 1573245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 999.81\n",
      "Num timesteps: 1583245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 918.78\n",
      "Num timesteps: 1593245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1178.91\n",
      "Num timesteps: 1603245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 721.28\n",
      "Num timesteps: 1613245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 931.07\n",
      "Num timesteps: 1623245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 844.45\n",
      "Num timesteps: 1633245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 854.73\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 716          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 800          |\n",
      "|    time_elapsed         | 2226         |\n",
      "|    total_timesteps      | 1638400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046550543 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.26        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.51e+03     |\n",
      "|    n_updates            | 112390       |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    std                  | 0.698        |\n",
      "|    value_loss           | 8.33e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1643245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 848.73\n",
      "Num timesteps: 1653245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 615.47\n",
      "Num timesteps: 1663245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 908.82\n",
      "Num timesteps: 1673245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 762.30\n",
      "Num timesteps: 1683245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 398.84\n",
      "Num timesteps: 1693245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 993.84\n",
      "Num timesteps: 1703245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 472.71\n",
      "Num timesteps: 1713245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 848.43\n",
      "Num timesteps: 1723245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 801.02\n",
      "Num timesteps: 1733245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 899.66\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 850         |\n",
      "|    time_elapsed         | 2361        |\n",
      "|    total_timesteps      | 1740800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003190497 |\n",
      "|    clip_fraction        | 0.0114      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.5e+03     |\n",
      "|    n_updates            | 112890      |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 1.64e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 1743245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 381.14\n",
      "Num timesteps: 1753245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1027.94\n",
      "Num timesteps: 1763245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 696.73\n",
      "Num timesteps: 1773245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1101.78\n",
      "Num timesteps: 1783245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 860.24\n",
      "Num timesteps: 1793245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1279.30\n",
      "Num timesteps: 1803245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1035.17\n",
      "Num timesteps: 1813245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 839.15\n",
      "Num timesteps: 1823245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 415.56\n",
      "Num timesteps: 1833245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 866.15\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 563          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 738          |\n",
      "|    iterations           | 900          |\n",
      "|    time_elapsed         | 2496         |\n",
      "|    total_timesteps      | 1843200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019417058 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.21        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.22e+03     |\n",
      "|    n_updates            | 113390       |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 1.05e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1843245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 554.80\n",
      "Num timesteps: 1853245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 721.52\n",
      "Num timesteps: 1863245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1123.85\n",
      "Num timesteps: 1873245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 893.91\n",
      "Num timesteps: 1883245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 684.04\n",
      "Num timesteps: 1893245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 773.81\n",
      "Num timesteps: 1903245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1120.79\n",
      "Num timesteps: 1913245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 915.97\n",
      "Num timesteps: 1923245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1073.12\n",
      "Num timesteps: 1933245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1063.95\n",
      "Num timesteps: 1943245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 673.56\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.11e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 738          |\n",
      "|    iterations           | 950          |\n",
      "|    time_elapsed         | 2635         |\n",
      "|    total_timesteps      | 1945600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041420083 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.2         |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.31e+03     |\n",
      "|    n_updates            | 113890       |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    std                  | 0.692        |\n",
      "|    value_loss           | 5.99e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 1953245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 596.18\n",
      "Num timesteps: 1963245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1272.03\n",
      "Num timesteps: 1973245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 458.03\n",
      "Num timesteps: 1983245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1188.37\n",
      "Num timesteps: 1993245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1243.78\n",
      "Num timesteps: 2003245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1379.73\n",
      "Num timesteps: 2013245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 855.04\n",
      "Num timesteps: 2023245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 655.80\n",
      "Num timesteps: 2033245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 567.03\n",
      "Num timesteps: 2043245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1005.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 985         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 738         |\n",
      "|    iterations           | 1000        |\n",
      "|    time_elapsed         | 2774        |\n",
      "|    total_timesteps      | 2048000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005718247 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.7e+03     |\n",
      "|    n_updates            | 114390      |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.69        |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 2053245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 804.78\n",
      "Num timesteps: 2063245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 358.41\n",
      "Num timesteps: 2073245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 642.40\n",
      "Num timesteps: 2083245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 626.93\n",
      "Num timesteps: 2093245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 896.39\n",
      "Num timesteps: 2103245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 566.15\n",
      "Num timesteps: 2113245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 602.71\n",
      "Num timesteps: 2123245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 228.95\n",
      "Num timesteps: 2133245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 720.06\n",
      "Num timesteps: 2143245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 873.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 912         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 1050        |\n",
      "|    time_elapsed         | 2916        |\n",
      "|    total_timesteps      | 2150400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008865711 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.44e+04    |\n",
      "|    n_updates            | 114890      |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 9.56e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 2153245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1075.79\n",
      "Num timesteps: 2163245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1018.39\n",
      "Num timesteps: 2173245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 802.65\n",
      "Num timesteps: 2183245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 746.92\n",
      "Num timesteps: 2193245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 781.05\n",
      "Num timesteps: 2203245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 930.75\n",
      "Num timesteps: 2213245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 787.78\n",
      "Num timesteps: 2223245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 970.43\n",
      "Num timesteps: 2233245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1292.14\n",
      "Num timesteps: 2243245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 685.10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 546          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 736          |\n",
      "|    iterations           | 1100         |\n",
      "|    time_elapsed         | 3057         |\n",
      "|    total_timesteps      | 2252800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047320724 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.15        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.49e+03     |\n",
      "|    n_updates            | 115390       |\n",
      "|    policy_gradient_loss | -0.00489     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 9.36e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 2253245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 554.55\n",
      "Num timesteps: 2263245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 842.61\n",
      "Num timesteps: 2273245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 862.86\n",
      "Num timesteps: 2283245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1043.31\n",
      "Num timesteps: 2293245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 894.22\n",
      "Num timesteps: 2303245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 845.25\n",
      "Num timesteps: 2313245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 970.55\n",
      "Num timesteps: 2323245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 871.22\n",
      "Num timesteps: 2333245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 799.01\n",
      "Num timesteps: 2343245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 904.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2353245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 646.26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 695         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1150        |\n",
      "|    time_elapsed         | 3197        |\n",
      "|    total_timesteps      | 2355200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006688437 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.14       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.76e+03    |\n",
      "|    n_updates            | 115890      |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 6.13e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 2363245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 512.48\n",
      "Num timesteps: 2373245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 894.61\n",
      "Num timesteps: 2383245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 908.31\n",
      "Num timesteps: 2393245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1169.77\n",
      "Num timesteps: 2403245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 841.74\n",
      "Num timesteps: 2413245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1120.86\n",
      "Num timesteps: 2423245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 880.77\n",
      "Num timesteps: 2433245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 862.48\n",
      "Num timesteps: 2443245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 836.05\n",
      "Num timesteps: 2453245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 891.01\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 536          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 1200         |\n",
      "|    time_elapsed         | 3339         |\n",
      "|    total_timesteps      | 2457600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056724297 |\n",
      "|    clip_fraction        | 0.0206       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.15        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.89e+03     |\n",
      "|    n_updates            | 116390       |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    std                  | 0.689        |\n",
      "|    value_loss           | 1.37e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 2463245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 822.53\n",
      "Num timesteps: 2473245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1168.81\n",
      "Num timesteps: 2483245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 998.71\n",
      "Num timesteps: 2493245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 707.68\n",
      "Num timesteps: 2503245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1108.67\n",
      "Num timesteps: 2513245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1071.39\n",
      "Num timesteps: 2523245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 405.22\n",
      "Num timesteps: 2533245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 962.40\n",
      "Num timesteps: 2543245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 849.42\n",
      "Num timesteps: 2553245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 572.04\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 979          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 1250         |\n",
      "|    time_elapsed         | 3479         |\n",
      "|    total_timesteps      | 2560000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036422417 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.14        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.48e+03     |\n",
      "|    n_updates            | 116890       |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    std                  | 0.688        |\n",
      "|    value_loss           | 2.08e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 2563245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 862.60\n",
      "Num timesteps: 2573245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1152.65\n",
      "Num timesteps: 2583245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1002.68\n",
      "Num timesteps: 2593245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1088.14\n",
      "Num timesteps: 2603245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 616.75\n",
      "Num timesteps: 2613245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 581.51\n",
      "Num timesteps: 2623245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1218.70\n",
      "Num timesteps: 2633245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1020.63\n",
      "Num timesteps: 2643245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 688.92\n",
      "Num timesteps: 2653245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 640.84\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 935          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 734          |\n",
      "|    iterations           | 1300         |\n",
      "|    time_elapsed         | 3627         |\n",
      "|    total_timesteps      | 2662400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027510126 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.12        |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.27e+04     |\n",
      "|    n_updates            | 117390       |\n",
      "|    policy_gradient_loss | -0.00376     |\n",
      "|    std                  | 0.686        |\n",
      "|    value_loss           | 1.09e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 2663245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 849.04\n",
      "Num timesteps: 2673245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1046.31\n",
      "Num timesteps: 2683245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 762.26\n",
      "Num timesteps: 2693245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 603.68\n",
      "Num timesteps: 2703245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 943.05\n",
      "Num timesteps: 2713245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1056.54\n",
      "Num timesteps: 2723245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 551.09\n",
      "Num timesteps: 2733245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1062.21\n",
      "Num timesteps: 2743245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1013.25\n",
      "Num timesteps: 2753245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 942.87\n",
      "Num timesteps: 2763245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 850.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 822          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 1350         |\n",
      "|    time_elapsed         | 3779         |\n",
      "|    total_timesteps      | 2764800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045855716 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.11        |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+03     |\n",
      "|    n_updates            | 117890       |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    std                  | 0.685        |\n",
      "|    value_loss           | 1.65e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2773245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 833.25\n",
      "Num timesteps: 2783245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 637.31\n",
      "Num timesteps: 2793245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1115.78\n",
      "Num timesteps: 2803245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 864.77\n",
      "Num timesteps: 2813245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1506.98\n",
      "Num timesteps: 2823245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 785.47\n",
      "Num timesteps: 2833245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 781.11\n",
      "Num timesteps: 2843245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 935.24\n",
      "Num timesteps: 2853245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 700.89\n",
      "Num timesteps: 2863245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 880.07\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.08e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 1400         |\n",
      "|    time_elapsed         | 3921         |\n",
      "|    total_timesteps      | 2867200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010195611 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.09        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+03     |\n",
      "|    n_updates            | 118390       |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    std                  | 0.685        |\n",
      "|    value_loss           | 9.27e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 2873245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 641.53\n",
      "Num timesteps: 2883245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 533.88\n",
      "Num timesteps: 2893245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1071.98\n",
      "Num timesteps: 2903245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1155.34\n",
      "Num timesteps: 2913245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1016.66\n",
      "Num timesteps: 2923245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 675.44\n",
      "Num timesteps: 2933245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1005.63\n",
      "Num timesteps: 2943245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 613.10\n",
      "Num timesteps: 2953245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 757.82\n",
      "Num timesteps: 2963245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 404.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 778         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 731         |\n",
      "|    iterations           | 1450        |\n",
      "|    time_elapsed         | 4061        |\n",
      "|    total_timesteps      | 2969600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007997924 |\n",
      "|    clip_fraction        | 0.0258      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.77e+03    |\n",
      "|    n_updates            | 118890      |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 1.22e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 2973245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 292.05\n",
      "Num timesteps: 2983245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 689.18\n",
      "Num timesteps: 2993245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1028.85\n",
      "Num timesteps: 3003245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 809.22\n",
      "Num timesteps: 3013245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 939.99\n",
      "Num timesteps: 3023245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 895.85\n",
      "Num timesteps: 3033245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 495.70\n",
      "Num timesteps: 3043245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1005.54\n",
      "Num timesteps: 3053245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 806.62\n",
      "Num timesteps: 3063245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1000.67\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 558          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 1500         |\n",
      "|    time_elapsed         | 4201         |\n",
      "|    total_timesteps      | 3072000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064540114 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.07        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+03     |\n",
      "|    n_updates            | 119390       |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    std                  | 0.683        |\n",
      "|    value_loss           | 8.13e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 3073245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 882.21\n",
      "Num timesteps: 3083245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1132.01\n",
      "Num timesteps: 3093245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 855.46\n",
      "Num timesteps: 3103245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 675.88\n",
      "Num timesteps: 3113245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 973.89\n",
      "Num timesteps: 3123245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 722.25\n",
      "Num timesteps: 3133245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 837.59\n",
      "Num timesteps: 3143245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1027.53\n",
      "Num timesteps: 3153245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 745.28\n",
      "Num timesteps: 3163245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 892.29\n",
      "Num timesteps: 3173245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 861.80\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36         |\n",
      "|    ep_rew_mean          | 1.04e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 1550       |\n",
      "|    time_elapsed         | 4341       |\n",
      "|    total_timesteps      | 3174400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00478998 |\n",
      "|    clip_fraction        | 0.0121     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.03      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.8e+03    |\n",
      "|    n_updates            | 119890     |\n",
      "|    policy_gradient_loss | -0.00192   |\n",
      "|    std                  | 0.678      |\n",
      "|    value_loss           | 5.63e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 3183245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 734.31\n",
      "Num timesteps: 3193245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 623.44\n",
      "Num timesteps: 3203245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 447.81\n",
      "Num timesteps: 3213245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1180.37\n",
      "Num timesteps: 3223245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 984.30\n",
      "Num timesteps: 3233245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 629.91\n",
      "Num timesteps: 3243245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 724.18\n",
      "Num timesteps: 3253245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1120.11\n",
      "Num timesteps: 3263245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 815.17\n",
      "Num timesteps: 3273245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 837.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.37e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 1600         |\n",
      "|    time_elapsed         | 4482         |\n",
      "|    total_timesteps      | 3276800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021060398 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.98        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22e+03     |\n",
      "|    n_updates            | 120390       |\n",
      "|    policy_gradient_loss | -0.000859    |\n",
      "|    std                  | 0.673        |\n",
      "|    value_loss           | 1.09e+04     |\n",
      "------------------------------------------\n",
      "Num timesteps: 3283245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 492.48\n",
      "Num timesteps: 3293245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 582.34\n",
      "Num timesteps: 3303245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 826.73\n",
      "Num timesteps: 3313245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1264.59\n",
      "Num timesteps: 3323245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1209.64\n",
      "Num timesteps: 3333245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 832.39\n",
      "Num timesteps: 3343245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1028.55\n",
      "Num timesteps: 3353245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 940.28\n",
      "Num timesteps: 3363245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1151.66\n",
      "Num timesteps: 3373245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 796.84\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 606          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 1650         |\n",
      "|    time_elapsed         | 4622         |\n",
      "|    total_timesteps      | 3379200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024982565 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.96        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.34e+03     |\n",
      "|    n_updates            | 120890       |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    std                  | 0.67         |\n",
      "|    value_loss           | 7.2e+03      |\n",
      "------------------------------------------\n",
      "Num timesteps: 3383245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1220.08\n",
      "Num timesteps: 3393245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1236.65\n",
      "Num timesteps: 3403245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 483.06\n",
      "Num timesteps: 3413245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1190.59\n",
      "Num timesteps: 3423245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 933.65\n",
      "Num timesteps: 3433245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1012.76\n",
      "Num timesteps: 3443245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1086.57\n",
      "Num timesteps: 3453245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 789.53\n",
      "Num timesteps: 3463245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1196.01\n",
      "Num timesteps: 3473245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 688.59\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.06e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 1700         |\n",
      "|    time_elapsed         | 4763         |\n",
      "|    total_timesteps      | 3481600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036347564 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.93        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.02e+03     |\n",
      "|    n_updates            | 121390       |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    std                  | 0.668        |\n",
      "|    value_loss           | 7.48e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 3483245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 979.99\n",
      "Num timesteps: 3493245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 997.91\n",
      "Num timesteps: 3503245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 502.93\n",
      "Num timesteps: 3513245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 920.78\n",
      "Num timesteps: 3523245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 829.91\n",
      "Num timesteps: 3533245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 878.97\n",
      "Num timesteps: 3543245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1088.09\n",
      "Num timesteps: 3553245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1175.76\n",
      "Num timesteps: 3563245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 854.25\n",
      "Num timesteps: 3573245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 804.76\n",
      "Num timesteps: 3583245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 872.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 991         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 1750        |\n",
      "|    time_elapsed         | 4903        |\n",
      "|    total_timesteps      | 3584000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004115176 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.56e+03    |\n",
      "|    n_updates            | 121890      |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 1.13e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 3593245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 710.07\n",
      "Num timesteps: 3603245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 659.54\n",
      "Num timesteps: 3613245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 985.44\n",
      "Num timesteps: 3623245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1008.81\n",
      "Num timesteps: 3633245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 845.90\n",
      "Num timesteps: 3643245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1010.79\n",
      "Num timesteps: 3653245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1060.68\n",
      "Num timesteps: 3663245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1010.10\n",
      "Num timesteps: 3673245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1204.08\n",
      "Num timesteps: 3683245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 771.75\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 234          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 1800         |\n",
      "|    time_elapsed         | 5043         |\n",
      "|    total_timesteps      | 3686400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050683934 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.84        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+03     |\n",
      "|    n_updates            | 122390       |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    std                  | 0.659        |\n",
      "|    value_loss           | 6.01e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3693245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 637.95\n",
      "Num timesteps: 3703245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 968.42\n",
      "Num timesteps: 3713245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1106.66\n",
      "Num timesteps: 3723245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 697.66\n",
      "Num timesteps: 3733245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 683.01\n",
      "Num timesteps: 3743245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1025.70\n",
      "Num timesteps: 3753245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 960.50\n",
      "Num timesteps: 3763245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 860.45\n",
      "Num timesteps: 3773245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 709.62\n",
      "Num timesteps: 3783245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 903.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 975         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 1850        |\n",
      "|    time_elapsed         | 5183        |\n",
      "|    total_timesteps      | 3788800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005653543 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.72e+03    |\n",
      "|    n_updates            | 122890      |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    std                  | 0.653       |\n",
      "|    value_loss           | 7.21e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 3793245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 798.36\n",
      "Num timesteps: 3803245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 965.01\n",
      "Num timesteps: 3813245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1526.18\n",
      "Num timesteps: 3823245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 963.86\n",
      "Num timesteps: 3833245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1077.21\n",
      "Num timesteps: 3843245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 811.59\n",
      "Num timesteps: 3853245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1099.19\n",
      "Num timesteps: 3863245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 889.83\n",
      "Num timesteps: 3873245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1384.10\n",
      "Num timesteps: 3883245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 530.67\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.3e+03      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 1900         |\n",
      "|    time_elapsed         | 5323         |\n",
      "|    total_timesteps      | 3891200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020620052 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.76        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.12e+03     |\n",
      "|    n_updates            | 123390       |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    std                  | 0.649        |\n",
      "|    value_loss           | 2.3e+04      |\n",
      "------------------------------------------\n",
      "Num timesteps: 3893245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 758.02\n",
      "Num timesteps: 3903245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 999.75\n",
      "Num timesteps: 3913245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 874.64\n",
      "Num timesteps: 3923245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 711.04\n",
      "Num timesteps: 3933245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1017.76\n",
      "Num timesteps: 3943245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1264.57\n",
      "Num timesteps: 3953245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 887.39\n",
      "Num timesteps: 3963245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 903.70\n",
      "Num timesteps: 3973245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 781.29\n",
      "Num timesteps: 3983245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 952.40\n",
      "Num timesteps: 3993245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1215.49\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36         |\n",
      "|    ep_rew_mean          | 1.11e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1950       |\n",
      "|    time_elapsed         | 5464       |\n",
      "|    total_timesteps      | 3993600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00638884 |\n",
      "|    clip_fraction        | 0.021      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.74      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.65e+03   |\n",
      "|    n_updates            | 123890     |\n",
      "|    policy_gradient_loss | -0.00346   |\n",
      "|    std                  | 0.649      |\n",
      "|    value_loss           | 5.24e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 4003245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 821.17\n",
      "Num timesteps: 4013245\n",
      "Best mean reward: 1551.89 - Last mean reward per episode: 1640.03\n",
      "Saving new best model to intra_tmp/best_model\n",
      "Num timesteps: 4023245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 977.21\n",
      "Num timesteps: 4033245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 332.72\n",
      "Num timesteps: 4043245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1039.98\n",
      "Num timesteps: 4053245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 596.19\n",
      "Num timesteps: 4063245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 782.15\n",
      "Num timesteps: 4073245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1418.15\n",
      "Num timesteps: 4083245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1361.66\n",
      "Num timesteps: 4093245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 778.90\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 987          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2000         |\n",
      "|    time_elapsed         | 5604         |\n",
      "|    total_timesteps      | 4096000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031805455 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.7         |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+03     |\n",
      "|    n_updates            | 124390       |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    std                  | 0.645        |\n",
      "|    value_loss           | 7.32e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 4103245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 701.36\n",
      "Num timesteps: 4113245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 752.91\n",
      "Num timesteps: 4123245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 935.69\n",
      "Num timesteps: 4133245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 339.37\n",
      "Num timesteps: 4143245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 686.37\n",
      "Num timesteps: 4153245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 858.60\n",
      "Num timesteps: 4163245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1024.49\n",
      "Num timesteps: 4173245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 512.01\n",
      "Num timesteps: 4183245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1046.43\n",
      "Num timesteps: 4193245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 935.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36         |\n",
      "|    ep_rew_mean          | 1.04e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 2050       |\n",
      "|    time_elapsed         | 5744       |\n",
      "|    total_timesteps      | 4198400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00621258 |\n",
      "|    clip_fraction        | 0.0277     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.66      |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.96e+03   |\n",
      "|    n_updates            | 124890     |\n",
      "|    policy_gradient_loss | -0.0046    |\n",
      "|    std                  | 0.641      |\n",
      "|    value_loss           | 6.62e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 4203245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 644.94\n",
      "Num timesteps: 4213245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 939.50\n",
      "Num timesteps: 4223245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1123.06\n",
      "Num timesteps: 4233245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 880.10\n",
      "Num timesteps: 4243245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 750.86\n",
      "Num timesteps: 4253245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 925.14\n",
      "Num timesteps: 4263245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1101.18\n",
      "Num timesteps: 4273245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 935.16\n",
      "Num timesteps: 4283245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 969.36\n",
      "Num timesteps: 4293245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 641.37\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 975          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2100         |\n",
      "|    time_elapsed         | 5884         |\n",
      "|    total_timesteps      | 4300800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047519505 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.65        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+03     |\n",
      "|    n_updates            | 125390       |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    std                  | 0.64         |\n",
      "|    value_loss           | 4.59e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 4303245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 906.86\n",
      "Num timesteps: 4313245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1270.65\n",
      "Num timesteps: 4323245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 953.87\n",
      "Num timesteps: 4333245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 773.18\n",
      "Num timesteps: 4343245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1087.22\n",
      "Num timesteps: 4353245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1168.00\n",
      "Num timesteps: 4363245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 862.78\n",
      "Num timesteps: 4373245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 822.22\n",
      "Num timesteps: 4383245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1152.66\n",
      "Num timesteps: 4393245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 674.36\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.33e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2150         |\n",
      "|    time_elapsed         | 6027         |\n",
      "|    total_timesteps      | 4403200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077249245 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.14e+03     |\n",
      "|    n_updates            | 125890       |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    std                  | 0.639        |\n",
      "|    value_loss           | 8.36e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 4403245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1312.15\n",
      "Num timesteps: 4413245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 961.13\n",
      "Num timesteps: 4423245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 861.42\n",
      "Num timesteps: 4433245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1014.85\n",
      "Num timesteps: 4443245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 867.38\n",
      "Num timesteps: 4453245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 734.31\n",
      "Num timesteps: 4463245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1369.18\n",
      "Num timesteps: 4473245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 715.93\n",
      "Num timesteps: 4483245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 717.08\n",
      "Num timesteps: 4493245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 626.03\n",
      "Num timesteps: 4503245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1111.26\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 856          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2200         |\n",
      "|    time_elapsed         | 6169         |\n",
      "|    total_timesteps      | 4505600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022243885 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.6         |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97e+03     |\n",
      "|    n_updates            | 126390       |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.636        |\n",
      "|    value_loss           | 4.66e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 4513245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 540.76\n",
      "Num timesteps: 4523245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 612.27\n",
      "Num timesteps: 4533245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 937.03\n",
      "Num timesteps: 4543245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1310.31\n",
      "Num timesteps: 4553245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 591.28\n",
      "Num timesteps: 4563245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 730.62\n",
      "Num timesteps: 4573245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1205.12\n",
      "Num timesteps: 4583245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 913.59\n",
      "Num timesteps: 4593245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1257.27\n",
      "Num timesteps: 4603245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1211.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 843          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2250         |\n",
      "|    time_elapsed         | 6310         |\n",
      "|    total_timesteps      | 4608000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041558896 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.58        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+03     |\n",
      "|    n_updates            | 126890       |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    std                  | 0.635        |\n",
      "|    value_loss           | 7.65e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4613245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1131.29\n",
      "Num timesteps: 4623245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 942.40\n",
      "Num timesteps: 4633245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1028.93\n",
      "Num timesteps: 4643245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 967.79\n",
      "Num timesteps: 4653245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1033.78\n",
      "Num timesteps: 4663245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1260.75\n",
      "Num timesteps: 4673245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 969.39\n",
      "Num timesteps: 4683245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 803.67\n",
      "Num timesteps: 4693245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 952.52\n",
      "Num timesteps: 4703245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 598.79\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 915         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 729         |\n",
      "|    iterations           | 2300        |\n",
      "|    time_elapsed         | 6454        |\n",
      "|    total_timesteps      | 4710400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009089576 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.54       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57e+03    |\n",
      "|    n_updates            | 127390      |\n",
      "|    policy_gradient_loss | -0.00483    |\n",
      "|    std                  | 0.631       |\n",
      "|    value_loss           | 4.77e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 4713245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 825.37\n",
      "Num timesteps: 4723245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 858.99\n",
      "Num timesteps: 4733245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 524.33\n",
      "Num timesteps: 4743245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 526.04\n",
      "Num timesteps: 4753245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 873.34\n",
      "Num timesteps: 4763245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 866.44\n",
      "Num timesteps: 4773245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1044.42\n",
      "Num timesteps: 4783245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1038.61\n",
      "Num timesteps: 4793245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1282.49\n",
      "Num timesteps: 4803245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1130.37\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.25e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 729          |\n",
      "|    iterations           | 2350         |\n",
      "|    time_elapsed         | 6599         |\n",
      "|    total_timesteps      | 4812800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064207944 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.49        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+03     |\n",
      "|    n_updates            | 127890       |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    std                  | 0.626        |\n",
      "|    value_loss           | 8.53e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 4813245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1173.52\n",
      "Num timesteps: 4823245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 944.43\n",
      "Num timesteps: 4833245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 865.63\n",
      "Num timesteps: 4843245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 768.55\n",
      "Num timesteps: 4853245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 824.83\n",
      "Num timesteps: 4863245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1214.51\n",
      "Num timesteps: 4873245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 904.16\n",
      "Num timesteps: 4883245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 841.77\n",
      "Num timesteps: 4893245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1240.61\n",
      "Num timesteps: 4903245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1237.50\n",
      "Num timesteps: 4913245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 666.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 713         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 2400        |\n",
      "|    time_elapsed         | 6745        |\n",
      "|    total_timesteps      | 4915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004659987 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.5e+03     |\n",
      "|    n_updates            | 128390      |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.624       |\n",
      "|    value_loss           | 4.76e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 4923245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 806.92\n",
      "Num timesteps: 4933245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1146.27\n",
      "Num timesteps: 4943245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 943.47\n",
      "Num timesteps: 4953245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1075.00\n",
      "Num timesteps: 4963245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1004.24\n",
      "Num timesteps: 4973245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 852.24\n",
      "Num timesteps: 4983245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1133.40\n",
      "Num timesteps: 4993245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 698.20\n",
      "Num timesteps: 5003245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 349.87\n",
      "Num timesteps: 5013245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 630.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 955         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 2450        |\n",
      "|    time_elapsed         | 6891        |\n",
      "|    total_timesteps      | 5017600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006156833 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.41       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.4e+03     |\n",
      "|    n_updates            | 128890      |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 0.619       |\n",
      "|    value_loss           | 6.62e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5023245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 904.75\n",
      "Num timesteps: 5033245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 707.83\n",
      "Num timesteps: 5043245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1081.02\n",
      "Num timesteps: 5053245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 864.81\n",
      "Num timesteps: 5063245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 910.51\n",
      "Num timesteps: 5073245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 958.67\n",
      "Num timesteps: 5083245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1070.23\n",
      "Num timesteps: 5093245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 741.58\n",
      "Num timesteps: 5103245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1181.00\n",
      "Num timesteps: 5113245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 781.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 620          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 2500         |\n",
      "|    time_elapsed         | 7035         |\n",
      "|    total_timesteps      | 5120000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045430087 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.37        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.06e+03     |\n",
      "|    n_updates            | 129390       |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 6.44e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5123245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1340.16\n",
      "Num timesteps: 5133245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 674.93\n",
      "Num timesteps: 5143245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 909.18\n",
      "Num timesteps: 5153245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1158.78\n",
      "Num timesteps: 5163245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 478.94\n",
      "Num timesteps: 5173245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 695.77\n",
      "Num timesteps: 5183245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1116.69\n",
      "Num timesteps: 5193245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1023.03\n",
      "Num timesteps: 5203245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1347.79\n",
      "Num timesteps: 5213245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1302.74\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.03e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 2550         |\n",
      "|    time_elapsed         | 7178         |\n",
      "|    total_timesteps      | 5222400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082164155 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.32        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+03     |\n",
      "|    n_updates            | 129890       |\n",
      "|    policy_gradient_loss | -0.00485     |\n",
      "|    std                  | 0.612        |\n",
      "|    value_loss           | 6.43e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5223245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 896.30\n",
      "Num timesteps: 5233245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 956.93\n",
      "Num timesteps: 5243245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1135.93\n",
      "Num timesteps: 5253245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 999.36\n",
      "Num timesteps: 5263245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 863.68\n",
      "Num timesteps: 5273245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1089.96\n",
      "Num timesteps: 5283245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1290.89\n",
      "Num timesteps: 5293245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 908.90\n",
      "Num timesteps: 5303245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 880.46\n",
      "Num timesteps: 5313245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 949.22\n",
      "Num timesteps: 5323245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1069.75\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 986          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 2600         |\n",
      "|    time_elapsed         | 7323         |\n",
      "|    total_timesteps      | 5324800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076116817 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.29        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 130390       |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    std                  | 0.609        |\n",
      "|    value_loss           | 6.55e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5333245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 982.11\n",
      "Num timesteps: 5343245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 882.89\n",
      "Num timesteps: 5353245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 665.26\n",
      "Num timesteps: 5363245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1176.03\n",
      "Num timesteps: 5373245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 380.70\n",
      "Num timesteps: 5383245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 742.05\n",
      "Num timesteps: 5393245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 709.43\n",
      "Num timesteps: 5403245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 918.04\n",
      "Num timesteps: 5413245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 817.22\n",
      "Num timesteps: 5423245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1025.79\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.09e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 726          |\n",
      "|    iterations           | 2650         |\n",
      "|    time_elapsed         | 7467         |\n",
      "|    total_timesteps      | 5427200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028135795 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.28        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+03     |\n",
      "|    n_updates            | 130890       |\n",
      "|    policy_gradient_loss | -0.00647     |\n",
      "|    std                  | 0.608        |\n",
      "|    value_loss           | 5.72e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5433245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1312.13\n",
      "Num timesteps: 5443245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 958.03\n",
      "Num timesteps: 5453245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 555.71\n",
      "Num timesteps: 5463245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1072.30\n",
      "Num timesteps: 5473245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 787.27\n",
      "Num timesteps: 5483245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 5493245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 762.74\n",
      "Num timesteps: 5503245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 678.82\n",
      "Num timesteps: 5513245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1135.50\n",
      "Num timesteps: 5523245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1208.99\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36         |\n",
      "|    ep_rew_mean          | 736        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 726        |\n",
      "|    iterations           | 2700       |\n",
      "|    time_elapsed         | 7612       |\n",
      "|    total_timesteps      | 5529600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00847077 |\n",
      "|    clip_fraction        | 0.0265     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.24      |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.02e+03   |\n",
      "|    n_updates            | 131390     |\n",
      "|    policy_gradient_loss | -0.00328   |\n",
      "|    std                  | 0.604      |\n",
      "|    value_loss           | 5.28e+03   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5533245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1193.78\n",
      "Num timesteps: 5543245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 919.58\n",
      "Num timesteps: 5553245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 469.19\n",
      "Num timesteps: 5563245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 837.15\n",
      "Num timesteps: 5573245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 802.34\n",
      "Num timesteps: 5583245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1223.28\n",
      "Num timesteps: 5593245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 894.70\n",
      "Num timesteps: 5603245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 510.57\n",
      "Num timesteps: 5613245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 803.19\n",
      "Num timesteps: 5623245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 922.28\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 371          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 2750         |\n",
      "|    time_elapsed         | 7762         |\n",
      "|    total_timesteps      | 5632000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022582994 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.19        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.48e+03     |\n",
      "|    n_updates            | 131890       |\n",
      "|    policy_gradient_loss | -0.00574     |\n",
      "|    std                  | 0.6          |\n",
      "|    value_loss           | 4.17e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5633245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 792.62\n",
      "Num timesteps: 5643245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 919.31\n",
      "Num timesteps: 5653245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1103.99\n",
      "Num timesteps: 5663245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1167.20\n",
      "Num timesteps: 5673245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 891.17\n",
      "Num timesteps: 5683245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1286.85\n",
      "Num timesteps: 5693245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 978.63\n",
      "Num timesteps: 5703245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1087.54\n",
      "Num timesteps: 5713245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1477.58\n",
      "Num timesteps: 5723245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1041.33\n",
      "Num timesteps: 5733245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1108.71\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 914          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 2800         |\n",
      "|    time_elapsed         | 7907         |\n",
      "|    total_timesteps      | 5734400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011459206 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.15        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69e+03     |\n",
      "|    n_updates            | 132390       |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    std                  | 0.596        |\n",
      "|    value_loss           | 4.52e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5743245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1158.94\n",
      "Num timesteps: 5753245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 663.46\n",
      "Num timesteps: 5763245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 797.15\n",
      "Num timesteps: 5773245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1048.57\n",
      "Num timesteps: 5783245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 700.59\n",
      "Num timesteps: 5793245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 972.13\n",
      "Num timesteps: 5803245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 874.03\n",
      "Num timesteps: 5813245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1172.57\n",
      "Num timesteps: 5823245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 976.08\n",
      "Num timesteps: 5833245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 884.59\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 781          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 2850         |\n",
      "|    time_elapsed         | 8051         |\n",
      "|    total_timesteps      | 5836800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048205825 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.09        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.87e+03     |\n",
      "|    n_updates            | 132890       |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 7.88e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5843245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 684.85\n",
      "Num timesteps: 5853245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1210.73\n",
      "Num timesteps: 5863245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1261.57\n",
      "Num timesteps: 5873245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1092.78\n",
      "Num timesteps: 5883245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 967.26\n",
      "Num timesteps: 5893245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 884.94\n",
      "Num timesteps: 5903245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 848.96\n",
      "Num timesteps: 5913245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1127.19\n",
      "Num timesteps: 5923245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1115.47\n",
      "Num timesteps: 5933245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 996.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 158          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 2900         |\n",
      "|    time_elapsed         | 8194         |\n",
      "|    total_timesteps      | 5939200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011882108 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.08        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.78e+03     |\n",
      "|    n_updates            | 133390       |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.591        |\n",
      "|    value_loss           | 5.25e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 5943245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1092.84\n",
      "Num timesteps: 5953245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1091.40\n",
      "Num timesteps: 5963245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 967.35\n",
      "Num timesteps: 5973245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1179.88\n",
      "Num timesteps: 5983245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 643.37\n",
      "Num timesteps: 5993245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 841.81\n",
      "Num timesteps: 6003245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1169.52\n",
      "Num timesteps: 6013245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1047.58\n",
      "Num timesteps: 6023245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 893.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6033245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1317.68\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.06e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 2950         |\n",
      "|    time_elapsed         | 8337         |\n",
      "|    total_timesteps      | 6041600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077681458 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.04        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+03     |\n",
      "|    n_updates            | 133890       |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    std                  | 0.587        |\n",
      "|    value_loss           | 9.76e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 6043245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 885.76\n",
      "Num timesteps: 6053245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 875.96\n",
      "Num timesteps: 6063245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1256.75\n",
      "Num timesteps: 6073245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1457.83\n",
      "Num timesteps: 6083245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1077.45\n",
      "Num timesteps: 6093245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1082.87\n",
      "Num timesteps: 6103245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 741.02\n",
      "Num timesteps: 6113245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 599.63\n",
      "Num timesteps: 6123245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 749.27\n",
      "Num timesteps: 6133245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 891.55\n",
      "Num timesteps: 6143245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1060.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1.02e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 3000        |\n",
      "|    time_elapsed         | 8476        |\n",
      "|    total_timesteps      | 6144000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007852616 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71e+03    |\n",
      "|    n_updates            | 134390      |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    std                  | 0.587       |\n",
      "|    value_loss           | 4.23e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6153245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 643.11\n",
      "Num timesteps: 6163245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1147.49\n",
      "Num timesteps: 6173245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 908.19\n",
      "Num timesteps: 6183245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 827.10\n",
      "Num timesteps: 6193245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 635.89\n",
      "Num timesteps: 6203245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 864.67\n",
      "Num timesteps: 6213245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 802.40\n",
      "Num timesteps: 6223245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 735.72\n",
      "Num timesteps: 6233245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 917.48\n",
      "Num timesteps: 6243245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 808.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 3050        |\n",
      "|    time_elapsed         | 8616        |\n",
      "|    total_timesteps      | 6246400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004908931 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.31e+03    |\n",
      "|    n_updates            | 134890      |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 8.72e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6253245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1267.59\n",
      "Num timesteps: 6263245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 980.14\n",
      "Num timesteps: 6273245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1078.68\n",
      "Num timesteps: 6283245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1357.52\n",
      "Num timesteps: 6293245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1017.77\n",
      "Num timesteps: 6303245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1231.10\n",
      "Num timesteps: 6313245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 603.68\n",
      "Num timesteps: 6323245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 811.28\n",
      "Num timesteps: 6333245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 852.94\n",
      "Num timesteps: 6343245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 982.84\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.13e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 3100         |\n",
      "|    time_elapsed         | 8761         |\n",
      "|    total_timesteps      | 6348800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053443667 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.97        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.94e+03     |\n",
      "|    n_updates            | 135390       |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.582        |\n",
      "|    value_loss           | 4.06e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 6353245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1101.26\n",
      "Num timesteps: 6363245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 386.19\n",
      "Num timesteps: 6373245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 763.70\n",
      "Num timesteps: 6383245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1447.31\n",
      "Num timesteps: 6393245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 687.48\n",
      "Num timesteps: 6403245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 916.72\n",
      "Num timesteps: 6413245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 810.53\n",
      "Num timesteps: 6423245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 710.24\n",
      "Num timesteps: 6433245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 557.90\n",
      "Num timesteps: 6443245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1109.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.14e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 3150         |\n",
      "|    time_elapsed         | 8903         |\n",
      "|    total_timesteps      | 6451200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070097903 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.95        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.89e+03     |\n",
      "|    n_updates            | 135890       |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    std                  | 0.581        |\n",
      "|    value_loss           | 4.76e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6453245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 800.21\n",
      "Num timesteps: 6463245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 865.11\n",
      "Num timesteps: 6473245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 879.60\n",
      "Num timesteps: 6483245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1108.52\n",
      "Num timesteps: 6493245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1210.01\n",
      "Num timesteps: 6503245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 810.87\n",
      "Num timesteps: 6513245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 966.83\n",
      "Num timesteps: 6523245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1246.41\n",
      "Num timesteps: 6533245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1353.36\n",
      "Num timesteps: 6543245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 771.07\n",
      "Num timesteps: 6553245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 647.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 721         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 3200        |\n",
      "|    time_elapsed         | 9043        |\n",
      "|    total_timesteps      | 6553600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013534461 |\n",
      "|    clip_fraction        | 0.0672      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 864         |\n",
      "|    n_updates            | 136390      |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    std                  | 0.578       |\n",
      "|    value_loss           | 4.75e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6563245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1012.41\n",
      "Num timesteps: 6573245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 915.46\n",
      "Num timesteps: 6583245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 556.57\n",
      "Num timesteps: 6593245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 552.25\n",
      "Num timesteps: 6603245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 862.92\n",
      "Num timesteps: 6613245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 514.78\n",
      "Num timesteps: 6623245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 892.82\n",
      "Num timesteps: 6633245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 718.77\n",
      "Num timesteps: 6643245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 636.48\n",
      "Num timesteps: 6653245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 894.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 740         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 3250        |\n",
      "|    time_elapsed         | 9182        |\n",
      "|    total_timesteps      | 6656000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002103025 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.88       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2e+03       |\n",
      "|    n_updates            | 136890      |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 7.86e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6663245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1156.93\n",
      "Num timesteps: 6673245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1194.20\n",
      "Num timesteps: 6683245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1014.18\n",
      "Num timesteps: 6693245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1037.32\n",
      "Num timesteps: 6703245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1084.28\n",
      "Num timesteps: 6713245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1273.24\n",
      "Num timesteps: 6723245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 951.30\n",
      "Num timesteps: 6733245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 802.35\n",
      "Num timesteps: 6743245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 958.69\n",
      "Num timesteps: 6753245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 978.73\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.1e+03      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 3300         |\n",
      "|    time_elapsed         | 9322         |\n",
      "|    total_timesteps      | 6758400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058049643 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.86        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+03     |\n",
      "|    n_updates            | 137390       |\n",
      "|    policy_gradient_loss | -0.00464     |\n",
      "|    std                  | 0.575        |\n",
      "|    value_loss           | 2.94e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 6763245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 567.75\n",
      "Num timesteps: 6773245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1277.23\n",
      "Num timesteps: 6783245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1128.62\n",
      "Num timesteps: 6793245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 977.90\n",
      "Num timesteps: 6803245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 893.42\n",
      "Num timesteps: 6813245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 953.07\n",
      "Num timesteps: 6823245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 950.44\n",
      "Num timesteps: 6833245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 844.77\n",
      "Num timesteps: 6843245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 972.01\n",
      "Num timesteps: 6853245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 853.82\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 749          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 3350         |\n",
      "|    time_elapsed         | 9461         |\n",
      "|    total_timesteps      | 6860800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073773954 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.82        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.78e+03     |\n",
      "|    n_updates            | 137890       |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    std                  | 0.572        |\n",
      "|    value_loss           | 3.41e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 6863245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 665.76\n",
      "Num timesteps: 6873245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1232.25\n",
      "Num timesteps: 6883245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1022.75\n",
      "Num timesteps: 6893245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 927.91\n",
      "Num timesteps: 6903245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 974.55\n",
      "Num timesteps: 6913245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 830.61\n",
      "Num timesteps: 6923245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 845.51\n",
      "Num timesteps: 6933245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 564.10\n",
      "Num timesteps: 6943245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 824.62\n",
      "Num timesteps: 6953245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 886.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 968         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 725         |\n",
      "|    iterations           | 3400        |\n",
      "|    time_elapsed         | 9600        |\n",
      "|    total_timesteps      | 6963200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008372452 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.89e+03    |\n",
      "|    n_updates            | 138390      |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 0.57        |\n",
      "|    value_loss           | 6.67e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6963245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1005.13\n",
      "Num timesteps: 6973245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1146.03\n",
      "Num timesteps: 6983245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 580.22\n",
      "Num timesteps: 6993245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1212.92\n",
      "Num timesteps: 7003245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1241.18\n",
      "Num timesteps: 7013245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 830.20\n",
      "Num timesteps: 7023245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 565.99\n",
      "Num timesteps: 7033245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 638.45\n",
      "Num timesteps: 7043245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1012.84\n",
      "Num timesteps: 7053245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 978.42\n",
      "Num timesteps: 7063245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 887.63\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.03e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 3450         |\n",
      "|    time_elapsed         | 9740         |\n",
      "|    total_timesteps      | 7065600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062733064 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.74        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61e+03     |\n",
      "|    n_updates            | 138890       |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.566        |\n",
      "|    value_loss           | 5.18e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 7073245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 922.26\n",
      "Num timesteps: 7083245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 856.39\n",
      "Num timesteps: 7093245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 765.62\n",
      "Num timesteps: 7103245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1021.35\n",
      "Num timesteps: 7113245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 932.66\n",
      "Num timesteps: 7123245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 967.54\n",
      "Num timesteps: 7133245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 994.95\n",
      "Num timesteps: 7143245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1113.16\n",
      "Num timesteps: 7153245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1146.32\n",
      "Num timesteps: 7163245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1024.07\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 975          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 3500         |\n",
      "|    time_elapsed         | 9879         |\n",
      "|    total_timesteps      | 7168000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054418975 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.7         |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+03     |\n",
      "|    n_updates            | 139390       |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    std                  | 0.563        |\n",
      "|    value_loss           | 3.58e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 7173245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1058.59\n",
      "Num timesteps: 7183245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1190.17\n",
      "Num timesteps: 7193245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1102.98\n",
      "Num timesteps: 7203245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 678.50\n",
      "Num timesteps: 7213245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 846.28\n",
      "Num timesteps: 7223245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 817.11\n",
      "Num timesteps: 7233245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1050.12\n",
      "Num timesteps: 7243245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1041.12\n",
      "Num timesteps: 7253245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 886.28\n",
      "Num timesteps: 7263245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1010.91\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36           |\n",
      "|    ep_rew_mean          | 1.03e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 3550         |\n",
      "|    time_elapsed         | 10018        |\n",
      "|    total_timesteps      | 7270400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021189447 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.67        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+03     |\n",
      "|    n_updates            | 139890       |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.561        |\n",
      "|    value_loss           | 4.87e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 7273245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1306.04\n",
      "Num timesteps: 7283245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 983.78\n",
      "Num timesteps: 7293245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1171.19\n",
      "Num timesteps: 7303245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1239.20\n",
      "Num timesteps: 7313245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 595.70\n",
      "Num timesteps: 7323245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1219.86\n",
      "Num timesteps: 7333245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1121.52\n",
      "Num timesteps: 7343245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1126.57\n",
      "Num timesteps: 7353245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 879.14\n",
      "Num timesteps: 7363245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 944.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1.11e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 725         |\n",
      "|    iterations           | 3600        |\n",
      "|    time_elapsed         | 10155       |\n",
      "|    total_timesteps      | 7372800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008739242 |\n",
      "|    clip_fraction        | 0.0792      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.65       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25e+03    |\n",
      "|    n_updates            | 140390      |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 3.8e+03     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7373245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 727.43\n",
      "Num timesteps: 7383245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 818.32\n",
      "Num timesteps: 7393245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 775.38\n",
      "Num timesteps: 7403245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 294.53\n",
      "Num timesteps: 7413245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1358.23\n",
      "Num timesteps: 7423245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 695.98\n",
      "Num timesteps: 7433245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 902.39\n",
      "Num timesteps: 7443245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 798.72\n",
      "Num timesteps: 7453245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1043.43\n",
      "Num timesteps: 7463245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1077.47\n",
      "Num timesteps: 7473245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1056.43\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 835         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 3650        |\n",
      "|    time_elapsed         | 10292       |\n",
      "|    total_timesteps      | 7475200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004858605 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.43e+03    |\n",
      "|    n_updates            | 140890      |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.557       |\n",
      "|    value_loss           | 7.44e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7483245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 774.44\n",
      "Num timesteps: 7493245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 892.78\n",
      "Num timesteps: 7503245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 727.94\n",
      "Num timesteps: 7513245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 815.79\n",
      "Num timesteps: 7523245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1072.78\n",
      "Num timesteps: 7533245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 981.62\n",
      "Num timesteps: 7543245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1153.53\n",
      "Num timesteps: 7553245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 890.37\n",
      "Num timesteps: 7563245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 587.87\n",
      "Num timesteps: 7573245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 937.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 991         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 3700        |\n",
      "|    time_elapsed         | 10429       |\n",
      "|    total_timesteps      | 7577600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009521794 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.6        |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.81e+03    |\n",
      "|    n_updates            | 141390      |\n",
      "|    policy_gradient_loss | -0.00483    |\n",
      "|    std                  | 0.556       |\n",
      "|    value_loss           | 5.06e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7583245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1243.53\n",
      "Num timesteps: 7593245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1011.08\n",
      "Num timesteps: 7603245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 809.78\n",
      "Num timesteps: 7613245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 629.39\n",
      "Num timesteps: 7623245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 812.19\n",
      "Num timesteps: 7633245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 220.10\n",
      "Num timesteps: 7643245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 870.12\n",
      "Num timesteps: 7653245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 728.64\n",
      "Num timesteps: 7663245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 437.97\n",
      "Num timesteps: 7673245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1357.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 745         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 3750        |\n",
      "|    time_elapsed         | 10567       |\n",
      "|    total_timesteps      | 7680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005972281 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.52       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.55e+04    |\n",
      "|    n_updates            | 141890      |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.55        |\n",
      "|    value_loss           | 6.48e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7683245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1068.57\n",
      "Num timesteps: 7693245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 800.57\n",
      "Num timesteps: 7703245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1541.14\n",
      "Num timesteps: 7713245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 995.53\n",
      "Num timesteps: 7723245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 650.64\n",
      "Num timesteps: 7733245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1128.02\n",
      "Num timesteps: 7743245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 753.92\n",
      "Num timesteps: 7753245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 919.85\n",
      "Num timesteps: 7763245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1070.08\n",
      "Num timesteps: 7773245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1072.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1.32e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 727         |\n",
      "|    iterations           | 3800        |\n",
      "|    time_elapsed         | 10704       |\n",
      "|    total_timesteps      | 7782400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008291889 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 142390      |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 0.549       |\n",
      "|    value_loss           | 3.16e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7783245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1375.43\n",
      "Num timesteps: 7793245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 887.94\n",
      "Num timesteps: 7803245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1196.00\n",
      "Num timesteps: 7813245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 781.24\n",
      "Num timesteps: 7823245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 702.24\n",
      "Num timesteps: 7833245\n",
      "Best mean reward: 1640.03 - Last mean reward per episode: 1719.86\n",
      "Saving new best model to intra_tmp/best_model\n",
      "Num timesteps: 7843245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1427.85\n",
      "Num timesteps: 7853245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1168.96\n",
      "Num timesteps: 7863245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 870.55\n",
      "Num timesteps: 7873245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 860.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7883245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1151.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 866         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 727         |\n",
      "|    iterations           | 3850        |\n",
      "|    time_elapsed         | 10842       |\n",
      "|    total_timesteps      | 7884800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008410096 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 836         |\n",
      "|    n_updates            | 142890      |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    std                  | 0.546       |\n",
      "|    value_loss           | 4.23e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7893245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1060.50\n",
      "Num timesteps: 7903245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 922.23\n",
      "Num timesteps: 7913245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 878.16\n",
      "Num timesteps: 7923245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 718.21\n",
      "Num timesteps: 7933245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1123.89\n",
      "Num timesteps: 7943245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1274.37\n",
      "Num timesteps: 7953245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1131.38\n",
      "Num timesteps: 7963245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 579.64\n",
      "Num timesteps: 7973245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 708.22\n",
      "Num timesteps: 7983245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1005.30\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 677         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 727         |\n",
      "|    iterations           | 3900        |\n",
      "|    time_elapsed         | 10986       |\n",
      "|    total_timesteps      | 7987200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010780619 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.4        |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 936         |\n",
      "|    n_updates            | 143390      |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    std                  | 0.543       |\n",
      "|    value_loss           | 2.45e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7993245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1038.21\n",
      "Num timesteps: 8003245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 982.90\n",
      "Num timesteps: 8013245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1258.67\n",
      "Num timesteps: 8023245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1246.06\n",
      "Num timesteps: 8033245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 842.11\n",
      "Num timesteps: 8043245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1059.22\n",
      "Num timesteps: 8053245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 1153.78\n",
      "Num timesteps: 8063245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 878.56\n",
      "Num timesteps: 8073245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 774.32\n",
      "Num timesteps: 8083245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 937.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 1e+03       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 3950        |\n",
      "|    time_elapsed         | 11130       |\n",
      "|    total_timesteps      | 8089600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014610531 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.3e+03     |\n",
      "|    n_updates            | 143890      |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 0.541       |\n",
      "|    value_loss           | 2.91e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 8093245\n",
      "Best mean reward: 1719.86 - Last mean reward per episode: 761.59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-222601b952e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-395c8d159730>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mstock_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stock_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mperc_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-395c8d159730>\u001b[0m in \u001b[0;36mget_stock_obs\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_past_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m#stack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3034\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3598\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3599\u001b[0m         \"\"\"\n\u001b[0;32m-> 3600\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3601\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3586\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 3587\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3588\u001b[0m         )\n\u001b[1;32m   3589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"take\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1475\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_slice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m             )\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice)\u001b[0m\n\u001b[1;32m   1435\u001b[0m                             \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m                         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m                         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         )\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0mmask_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m         \u001b[0;31m# Check for EA to catch DatetimeArray, TimedeltaArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/envs/venv-884/lib/python3.7/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"__instancecheck__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__subclasscheck__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(5e7), callback = callback, log_interval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b20c9aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trade_env = TradeEnv(tickers=tickers)\n",
    "obs = trade_env.reset()\n",
    "index = trade_env.index\n",
    "done = False\n",
    "net_worth = [trade_env.balance]\n",
    "holdings = [obs[-6:]]\n",
    "actions = []\n",
    "while not done:\n",
    "    action = model.predict(obs, deterministic = False)[0] \n",
    "    actions.append(action)\n",
    "    obs, _, done, _ = trade_env.step(action)\n",
    "    net_worth.append(trade_env.net_worth)\n",
    "    holdings.append(obs[-6:])\n",
    "    \n",
    "df = trade_env.data\n",
    "\n",
    "for key in df.keys():\n",
    "    plt.plot(df.index, 1000.0*df[key]/df.loc[0, key], label = key)\n",
    "plt.plot(np.arange(len(net_worth)) + 3, net_worth, label = 'net_worth')\n",
    "plt.legend()\n",
    "actions = np.array(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7294318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x138314198>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.plot(np.arange(len(actions)), actions[:,i], label = 'a{}'.format(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b186750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6b4d57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df[200:230][tickers].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "dc2df330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0 [0.         2.91721511 0.         0.         0.        ]\n",
      "1075.3154672968517 [0.         0.         1.73844193 0.         0.        ]\n",
      "1289.9219070677075 [0.         4.63076049 0.         0.         0.        ]\n",
      "1603.5601503834052 [3.39950433 0.         0.         0.         0.        ]\n",
      "1761.698610802555 [3.39950433 0.         0.         0.         0.        ]\n",
      "2012.257635717981 [0.        7.2229094 0.        0.        0.       ]\n",
      "2526.073796749632 [0.         0.         4.70346114 0.         0.        ]\n",
      "3145.653309069966 [0.         0.         0.         0.         6.02761546]\n",
      "3912.761368049932 [ 0.         14.03494135  0.          0.          0.        ]\n",
      "4956.070609392556 [0.         0.         0.         0.         9.41441461]\n",
      "6094.211657096649 [ 0.          0.          0.         13.33043235  0.        ]\n",
      "6218.380236217108 [ 0.         22.27719023  0.          0.          0.        ]\n",
      "7940.820914839412 [16.71203385  0.          0.          0.          0.        ]\n",
      "8694.159391181654 [16.71203385  0.          0.          0.          0.        ]\n",
      "9937.806907107635 [ 0.        35.5374473  0.         0.         0.       ]\n",
      "12783.443604093432 [ 0.        35.5374473  0.         0.         0.       ]\n",
      "12812.113271083337 [ 0.          0.          0.          0.         23.48978065]\n",
      "14203.11877189117 [ 0.         50.66978825  0.          0.          0.        ]\n",
      "18388.224148762303 [32.58466855  0.          0.          0.          0.        ]\n",
      "19602.72972815875 [ 0.          0.         31.70788566  0.          0.        ]\n",
      "22613.179770161903 [ 0.         80.43693391  0.          0.          0.        ]\n",
      "29440.28567149481 [61.50861994  0.          0.          0.          0.        ]\n",
      "32123.063576039553 [ 0.          0.         55.57553337  0.          0.        ]\n",
      "37947.14816767307 [  0.         134.51263308   0.           0.           0.        ]\n",
      "49637.154804582606 [ 0.          0.         84.81565495  0.          0.        ]\n",
      "60616.56458999126 [ 0.          0.         84.81565495  0.          0.        ]\n",
      "62712.25124712117 [  0.         221.40721298   0.           0.           0.        ]\n",
      "82347.32464719925 [  0.           0.         121.63278588   0.           0.        ]\n",
      "95452.64949647107 [  0.           0.           0.           0.         178.04881675]\n"
     ]
    }
   ],
   "source": [
    "balance = 1000.0\n",
    "holdings = np.zeros(len(tickers))\n",
    "hold_cash = False\n",
    "for i in range(29):\n",
    "    if i == 0 or hold_cash:\n",
    "        balance = 1000.0\n",
    "    else:\n",
    "        balance = np.dot(holdings, arr[i])\n",
    "    holdings = np.zeros(len(tickers))\n",
    "    perc_inc = np.divide(arr[i+1] - arr[i], arr[i])\n",
    "    if np.max(perc_inc) > 0:\n",
    "        idx = np.argmax(perc_inc)\n",
    "        holdings[idx] += balance/arr[i][idx]\n",
    "        hold_cash = False\n",
    "    else:\n",
    "        hold_cash = True\n",
    "    print (balance, holdings)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "533d0790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[560.16479637, 342.79268526, 704.87633664, 545.53959789,\n",
       "        600.84992606],\n",
       "       [594.30388253, 368.61027654, 618.55127193, 520.73458222,\n",
       "        591.79635869],\n",
       "       [528.4260227 , 278.5550907 , 741.99884645, 542.50774401,\n",
       "        565.79206459],\n",
       "       [471.70410605, 346.28440736, 635.43103391, 518.56988114,\n",
       "        582.58603904],\n",
       "       [518.22219961, 367.19192545, 607.01785161, 448.16028293,\n",
       "        556.01961597],\n",
       "       [591.92677554, 278.59378042, 662.81233043, 497.21509162,\n",
       "        545.34508791],\n",
       "       [571.96149444, 349.73078802, 537.06700657, 539.13800242,\n",
       "        597.28111478],\n",
       "       [491.83617935, 365.66753092, 668.79542814, 456.74815145,\n",
       "        521.87358806],\n",
       "       [484.20325771, 278.78715497, 669.05960483, 436.31169961,\n",
       "        649.13918151],\n",
       "       [562.24905614, 353.12371367, 659.09377898, 476.67604727,\n",
       "        526.43428346],\n",
       "       [597.96298222, 364.04357003, 790.27902923, 457.1653416 ,\n",
       "        647.32773195],\n",
       "       [533.09657289, 279.13664929, 660.19542166, 466.48001167,\n",
       "        546.72827853],\n",
       "       [475.15586592, 356.45522765, 703.29432838, 475.07921064,\n",
       "        587.66559378],\n",
       "       [520.23347186, 362.32681209, 680.22876689, 411.55794184,\n",
       "        572.685115  ],\n",
       "       [594.64975938, 279.64324005, 565.96645815, 437.2158524 ,\n",
       "        535.99096367],\n",
       "       [576.37550176, 359.71755357, 690.51856894, 517.40873598,\n",
       "        597.74574466],\n",
       "       [496.13374716, 360.52429882, 610.28518513, 468.18206492,\n",
       "        545.43349985],\n",
       "       [486.76881313, 280.30744282, 661.24653484, 428.34633169,\n",
       "        604.65097495],\n",
       "       [564.32135014, 362.9031181 , 739.88427171, 487.63132339,\n",
       "        591.80293454],\n",
       "       [601.5936513 , 358.64332367, 618.22885123, 497.63965979,\n",
       "        582.05341867],\n",
       "       [537.76908074, 281.1293105 , 713.17211153, 498.90406065,\n",
       "        611.24983815],\n",
       "       [478.63674553, 366.00457326, 624.31463431, 532.2224691 ,\n",
       "        555.29667901],\n",
       "       [522.25303714, 356.69141036, 578.00729258, 491.8216686 ,\n",
       "        581.90891248],\n",
       "       [597.34654277, 282.10843323, 682.80313058, 489.194081  ,\n",
       "        568.69406465],\n",
       "       [580.77199299, 369.01481793, 585.23576616, 582.69353189,\n",
       "        540.05379229],\n",
       "       [500.45134941, 354.67629085, 714.68604032, 570.13110387,\n",
       "        624.21922837],\n",
       "       [489.35893714, 283.24393954, 739.39476488, 503.8106656 ,\n",
       "        524.38246193],\n",
       "       [566.3822583 , 371.92701872, 677.015856  , 549.28586405,\n",
       "        664.40754983],\n",
       "       [605.19574377, 352.60588265, 784.76085876, 578.92441933,\n",
       "        536.10381263],\n",
       "       [542.44291492, 284.53449895, 639.62296837, 558.74424583,\n",
       "        639.85903863]])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765604c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e5e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
